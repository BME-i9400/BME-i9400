{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5363f886",
   "metadata": {},
   "source": [
    "## BME i9400\n",
    "## Fall 2025\n",
    "\n",
    "### Homework 2: Logistic Regression with L1 and L2 Regularization\n",
    "\n",
    "**Assigned:** 2025-11-10  \n",
    "**Due:** 2025-11-24 11:59:59.999 PM EST\n",
    "\n",
    "**Place completed notebook into your \"my-work\" folder on JupyterHub**\n",
    "\n",
    "**Honor & AI use policy (read carefully)**\n",
    "- You may use docs/StackOverflow for syntax.  \n",
    "- You **may** ask an LLM for debugging/snippets, but you must include an **AI Log** at the end (prompts + what you used).\n",
    "- You **may not** ask for a full solution. Your code and plots must reflect your own understanding.\n",
    "\n",
    "**Deliverables**\n",
    "1. Executed notebook (.ipynb) with all cells run. \n",
    "2. `ai_log.md` (if you used an LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcb790",
   "metadata": {},
   "source": [
    "### Student Starter Notebook \n",
    "\n",
    "This notebook walks you step-by-step through the assignment.\n",
    "\n",
    "**Dataset assumptions:**\n",
    "- File: `diabetes.csv` (provided on GitHub)\n",
    "- Target column: `Outcome` (1 = diabetes, 0 = no diabetes)\n",
    "- All other columns are numeric predictors.\n",
    "\n",
    "**Instructions:**\n",
    "- Read the markdown **before** each code cell.\n",
    "- Fill in any `TODO` sections in the code.\n",
    "- Answer the short written questions in the markdown blocks.\n",
    "- Run cells in order so variables are defined correctly.\n",
    "\n",
    "Do **not** remove cells; add new ones if you want extra exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad118a30",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports\n",
    "\n",
    "Import all necessary libraries and set up basic plotting defaults.\n",
    "Run this cell first. No edits needed unless you add extra packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3579e9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Task 1 – Data Loading & Basic Exploration (10 pts)\n",
    "\n",
    "**Goal:** Load the dataset and understand what you are modeling.\n",
    "\n",
    "### Instructions\n",
    "- Load `diabetes.csv` into a DataFrame called `df`.\n",
    "- Separate features `X` and labels `y` using `Outcome` as the target column.\n",
    "- Print:\n",
    "  - The first 5 rows\n",
    "  - Number of samples and features\n",
    "  - Class counts and class proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "\n",
    "\n",
    "# TODO: Print the first few rows\n",
    "\n",
    "\n",
    "# TODO: Define features X and target y\n",
    "\n",
    "\n",
    "# TODO: Print the number of samples and features\n",
    "\n",
    "\n",
    "# TODO: Print the class counts and proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492c698",
   "metadata": {},
   "source": [
    "### Short Answer\n",
    "\n",
    "**(a)** Is the dataset balanced? Justify with the proportions.\n",
    "\n",
    "**(b)** Give 1–2 reasons why logistic regression is a reasonable model choice for this prediction task.\n",
    "\n",
    "> TODO: Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ada24",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Task 2 – Baseline Logistic Regression with Pipeline (20 pts)\n",
    "\n",
    "**Goal:** Train a clean baseline model that correctly handles scaling and evaluation.\n",
    "\n",
    "### Instructions\n",
    "- Create a train/test split:\n",
    "  - `test_size = 0.2`\n",
    "  - `stratify = y`\n",
    "  - `random_state = 9400`\n",
    "- Build a `Pipeline` with:\n",
    "  - `StandardScaler()`\n",
    "  - `LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=9400)`\n",
    "- Fit on the training data.\n",
    "- Report training and test accuracy.\n",
    "- Plot the confusion matrix on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e164e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train/test split\n",
    "\n",
    "\n",
    "# TODO: Define the baseline pipeline\n",
    "\n",
    "\n",
    "# TODO: Fit and evaluate\n",
    "\n",
    "\n",
    "# TODO: Report training and test accuracy\n",
    "\n",
    "\n",
    "# TODO: Plot Confusion matrix on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325c758",
   "metadata": {},
   "source": [
    "### Short Answer\n",
    "\n",
    "- Compare training vs test accuracy.\n",
    "- Does the model appear to overfit, underfit, or behave reasonably? Explain briefly.\n",
    "\n",
    "> TODO: Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba390251",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Task 3 – Effect of Regularization Strength C (25 pts)\n",
    "\n",
    "**Goal:** Use cross-validation to choose the L2 regularization strength.\n",
    "\n",
    "### Instructions\n",
    "- Use a `Pipeline` with `StandardScaler` + `LogisticRegression(penalty='l2')`.\n",
    "- Use `StratifiedKFold(n_splits=5, shuffle=True, random_state=9400)`.\n",
    "- Search `C ∈ {1e-3, 1e-2, 1e-1, 1, 10, 100}` with `GridSearchCV`.\n",
    "- For each C, record mean CV accuracy.\n",
    "- Plot mean CV accuracy vs `log10(C)`.\n",
    "- Refit the best model on the full training set and evaluate on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c90dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define grid of C values\n",
    "\n",
    "# TODO: Create pipeline for L2-regularized logistic regression\n",
    "\n",
    "# TODO: Search over grid of C parameters \n",
    "\n",
    "## TODO: For each C, record mean CV accuracy\n",
    "\n",
    "## TODO: Plot mean CV accuracy vs log10(C)\n",
    "\n",
    "## TODO: Refit best model on full training set & evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f973cc9",
   "metadata": {},
   "source": [
    "### Short Answer\n",
    "\n",
    "- How does C influence cross-validated performance?\n",
    "- What happens for very small vs very large C?\n",
    "- Which C do you choose, and why?\n",
    "\n",
    "> TODO: Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09722977",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Task 4 – L1 regularized logistic regression (25 points)\n",
    "\n",
    "### Instructions\n",
    "- Fit an L1-penalized model (`penalty='l1'`, `solver='liblinear'`) with GridSearchCV over the same C grid.\n",
    "- For the best model, report:\n",
    "  - Best hyperparameters\n",
    "  - Test accuracy\n",
    "  - Number of non-zero coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: count non-zero coefficients in a Pipeline(LogisticRegression)\n",
    "def count_nonzero_coefs(pipe_model):\n",
    "    logreg = pipe_model.named_steps['logreg']\n",
    "    return np.sum(logreg.coef_ != 0)\n",
    "\n",
    "# L2 summary (reuse best_l2_model)\n",
    "l2_nonzero = count_nonzero_coefs(best_l2_model)\n",
    "l2_test_acc = accuracy_score(y_test, best_l2_model.predict(X_test))\n",
    "print(f'[L2] best C={best_l2_model.named_steps[\"logreg\"].C}, '\n",
    "      f'non-zero coeffs={l2_nonzero}, test acc={l2_test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e97251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  redefine grid of C values\n",
    "\n",
    "# TODO: Create pipeline for L1-penalized logistic regression\n",
    "\n",
    "# TODO: fit pipeline and find best C value, measure test accuracy and number of non-zero coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39235c82",
   "metadata": {},
   "source": [
    "### Short Answer\n",
    "\n",
    "- Which penalty produced the sparsest model (fewest non-zero coefficients)?\n",
    "- How do test accuracies compare between L2 and L1?\n",
    "- If two models have similar accuracy but different sparsity, which would you prefer here, and why?\n",
    "\n",
    "> TODO: Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c34cc",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Task 5 – Probabilities, ROC Curve & Thresholds (20 pts)\n",
    "\n",
    "**Goal:** Use logistic regression calibration to study trade-offs between sensitivity and specificity.\n",
    "\n",
    "### Instructions\n",
    "- Using your **best L2 model**, compute predicted probabilities `P(Outcome=1)` on `X_test`.\n",
    "- Compute **ROC AUC** and plot the ROC curve.\n",
    "- Choose a threshold different from 0.5 (e.g., 0.4) and:\n",
    "  - Compute accuracy.\n",
    "  - Show the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute predicted probabilities for the positive class using the best L2 model\n",
    "\n",
    "\n",
    "# TODO: plot ROC curve and compute the area under the curve (AUC)\n",
    "# Hint: use sklearn.metrics.roc_curve and sklearn.metrics.roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Using a threshold of 0.4, compute accuracy and show confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cc712",
   "metadata": {},
   "source": [
    "### Short Answer\n",
    "\n",
    "- How does lowering the threshold (e.g., from 0.5 to 0.4) affect false negatives vs false positives?\n",
    "- For a diabetes screening scenario, would you choose a higher or lower threshold than 0.5? Explain.\n",
    "\n",
    "> TODO: Write your answer here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmei9400b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
