{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME I9400 – **HW3: Multilayer Neural Nets in PyTorch (Fashion-MNIST)**\n",
    "### Student Starter Notebook\n",
    "\n",
    "**What you will learn**\n",
    "- Build, train, and evaluate multilayer perceptrons (MLPs) in PyTorch\n",
    "- Use `Dataset`/`DataLoader`, reproducibility, device (CPU/GPU), and checkpoints\n",
    "- Diagnose under/overfitting; apply regularization (dropout, weight decay)\n",
    "- Do a small hyperparameter search\n",
    "\n",
    "**Dataset**: **Fashion-MNIST** (10 classes, 28×28 grayscale). Downloaded via `torchvision`.\n",
    "\n",
    "**References (course materials)**\n",
    "- Lecture 09: *Multilayer Perceptrons*: `lecture_09_multilayer_perceptrons.ipynb`\n",
    "- Lecture 10: *Neural Nets in PyTorch*: `lecture_10_neural_nets_in_pytorch.ipynb`\n",
    "\n",
    "**What to submit**\n",
    "- This notebook with **all TODOs completed** and all cells runnable top-to-bottom.\n",
    "- **Save the notebook to your my-work folder**.\n",
    "\n",
    "**Grading (100 pts)**\n",
    "- (10) Reproducible setup + clean data pipeline\n",
    "- (20) Baseline MLP implemented + trained with curves\n",
    "- (35) Capacity & regularization comparison (3 configs + analysis)\n",
    "- (35) Small hyperparameter search (design + results + chosen best)\n",
    "\n",
    "---\n",
    "**Checklist before you submit**\n",
    "- [ ] Every section’s `TODO` is completed\n",
    "- [ ] Plots are readable and titled\n",
    "- [ ] You wrote the short answers where requested\n",
    "- [ ] Notebook runs from top to bottom without errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup, reproducibility, and device\n",
    "**Instructions**\n",
    "- Run this cell first. Do not change the seeding unless you document why.\n",
    "- This cell sets deterministic flags to help reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int = 9400):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(9400)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data pipeline (Fashion-MNIST)\n",
    "**Goal:** Build train/val/test dataloaders with appropriate normalization.\n",
    "\n",
    "**Instructions**\n",
    "- Use `FashionMNIST` via `torchvision.datasets`.\n",
    "- Normalize with **mean≈0.286** and **std≈0.353** (already provided below).\n",
    "- Split the training set into **50k train / 10k val** deterministically.\n",
    "- Use `shuffle=True` for the train loader.\n",
    "- Print one batch shape to confirm `[B,1,28,28]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell\n",
    "DATA_DIR = './data'\n",
    "BATCH_SIZE = 128  # you may adjust if you have limited compute\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.286,), (0.353,)),\n",
    "])\n",
    "\n",
    "full_train = datasets.FashionMNIST(DATA_DIR, train=True, download=True, transform=transform)\n",
    "test_ds   = datasets.FashionMNIST(DATA_DIR, train=False, download=True, transform=transform)\n",
    "\n",
    "train_size = 50000\n",
    "val_size = len(full_train) - train_size\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size], generator=torch.Generator().manual_seed(9400))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print('Batch shapes:', xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short answer (1–2 sentences):** Why do we normalize images before training?\n",
    "\n",
    "> TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Baseline MLP + training loop\n",
    "**Goal:** Implement a clean baseline MLP and a minimal training/eval loop, and save the best checkpoint.\n",
    "\n",
    "**Specs**\n",
    "- Model: `Flatten → Linear(784→256) → ReLU → Linear(256→10)`.\n",
    "- Loss: `CrossEntropyLoss`; Optimizer: `Adam(lr=1e-3)`; Epochs: **10** (you may increase if you have time).\n",
    "- Save the **best-val** checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the forward() method below\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=28*28, hidden_dim=256, num_classes=10, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        raise NotImplementedError('TODO: Implement forward pass')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement accuracy_from_logits() method, which returns scalar float accuracy in [0,1]\n",
    "# the inputs are the outputs of the model (called logits) and the targets\n",
    "# Hint: use torch.argmax() to get the predicted class\n",
    "\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    \"\"\"Return scalar float accuracy in [0,1].\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the evaluate_epoch() method below\n",
    "# Hint: this is very similar to the train_epoch() method below (code provided)\n",
    "# The only difference is that we don't need to compute the gradients\n",
    "# You will need to set the model to evaluation mode\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Iterate loader in eval mode; return (avg_loss, avg_acc).\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = accuracy_from_logits(logits, yb)\n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc += acc * bs\n",
    "        n += bs\n",
    "    return total_loss / n, total_acc / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the cell to train your baseline model and save best checkpoint\n",
    "EPOCHS = 10\n",
    "model = MLP(hidden_dim=256, dropout=0.0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "ckpt_path = 'best_mlp_baseline.pt'\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    history['train_loss'].append(tr_loss); history['train_acc'].append(tr_acc)\n",
    "    history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "    print(f'Epoch {epoch:02d}: train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={val_loss:.4f} acc={val_acc:.3f}')\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "print('Saved best baseline to', ckpt_path)\n",
    "\n",
    "plt.figure(); plt.plot(history['train_loss'], label='train'); plt.plot(history['val_loss'], label='val');\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Baseline MLP – Loss'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(); plt.plot(history['train_acc'], label='train'); plt.plot(history['val_acc'], label='val');\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Baseline MLP – Accuracy'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short answer (2–3 sentences):** Based on the curves, does the baseline underfit, overfit, or generalize well? Explain briefly.\n",
    "\n",
    "> TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Capacity & regularization study\n",
    "**Goal:** Compare three configurations for ~10 epochs each and discuss bias/variance trade-offs.\n",
    "\n",
    "**Configs**\n",
    "1. `1×128` (no dropout)\n",
    "2. `2×256` (no dropout)\n",
    "3. `2×256` + `dropout=0.3` + `weight_decay=1e-4`\n",
    "\n",
    "**Deliverables**\n",
    "- Val accuracy curves for all three\n",
    "- A short paragraph: which overfits/underfits and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell\n",
    "# Helper model for deeper networks\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, hidden_sizes=(128,), dropout=0.0, num_classes=10):\n",
    "        super().__init__()\n",
    "        layers = [nn.Flatten()]\n",
    "        in_dim = 28*28\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU()]\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def fit_model(model, train_loader, val_loader, epochs, optimizer, criterion, device):\n",
    "    hist = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    for _ in range(epochs):\n",
    "        tl, ta = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        vl, va = evaluate_epoch(model, val_loader, criterion, device)\n",
    "        hist['train_loss'].append(tl); hist['train_acc'].append(ta)\n",
    "        hist['val_loss'].append(vl); hist['val_acc'].append(va)\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the three configs defined here to evaluate 3 different models\n",
    "configs = {\n",
    "    'small_1x128': dict(hidden_sizes=(128,), dropout=0.0, weight_decay=0.0),\n",
    "    'medium_2x256': dict(hidden_sizes=(256,256), dropout=0.0, weight_decay=0.0),\n",
    "    'medium_reg':   dict(hidden_sizes=(256,256), dropout=0.3, weight_decay=1e-4),\n",
    "}\n",
    "\n",
    "# TODO: fill in the required lines in the for loop below\n",
    "results = {}\n",
    "for name, cfg in configs.items():\n",
    "    print('\\nRunning', name, cfg)\n",
    "    \n",
    "    # TODO: create DeepMLP model  \n",
    "    \n",
    "    # TODO: create Adam optimizer\n",
    "    \n",
    "    # TODO: fit the model\n",
    "    \n",
    "    # TODO: store the results\n",
    "\n",
    "# TODO: Plot the validation accuracy versus epoch for all three configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short answer (3–4 sentences):** Which config overfits the most? Which underfits? Explain using the curves and final metrics.\n",
    "\n",
    "> TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Small hyperparameter search (use 2×256)\n",
    "**Goal:** Tune a few hyperparameters quickly and pick a winner by validation accuracy.\n",
    "\n",
    "**Grid**\n",
    "- `lr ∈ {1e-3, 3e-4}`\n",
    "- `batch_size ∈ {64, 128}`\n",
    "- `dropout ∈ {0.0, 0.3}`\n",
    "- `weight_decay ∈ {0.0, 1e-4}`\n",
    "\n",
    "**Instructions**\n",
    "- Train each candidate for ~8 epochs\n",
    "- Report the **best config** and its max val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided code excerpts to implement a small hyperparameter search over dropout, learning rate, batch size, and weight decay\n",
    "from itertools import product\n",
    "\n",
    "def rebuild_loaders(bs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=2, pin_memory=True),\n",
    "            DataLoader(val_ds,   batch_size=bs, shuffle=False, num_workers=2, pin_memory=True))\n",
    "\n",
    "# This initializes the best configuration and the best validation accuracy\n",
    "best_cfg, best_acc = None, -1\n",
    "# Note: best_cfg is a dictionary that will store the best configuration found during the search {lr: ___, bs: ___, dropout: ___, weight_decay: ___}\n",
    "\n",
    "# TODO: fill in the missing lines in the for loop below\n",
    "for lr, bs, dp, wd in product([1e-3, 3e-4], [64, 128], [0.0, 0.3], [0.0, 1e-4]):\n",
    "    print(f'\\nTrial: lr={lr} bs={bs} dropout={dp} wd={wd}')\n",
    "    tr_loader, v_loader = rebuild_loaders(bs)\n",
    "\n",
    "    # TODO: create a DeepMLP model with the correct value for dropout\n",
    "\n",
    "    # TODO: create an Adam optimizer with the correct value for learning rate and weight decay\n",
    "\n",
    "    # TODO: fit the model\n",
    "\n",
    "    # TODO: find the maximum validation accuracy\n",
    "\n",
    "    # TODO: update the best configuration if the current one has a higher validation accuracy\n",
    "\n",
    "print('\\nBest config:', best_cfg, 'val_acc=', best_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short answer (2–4 sentences):** Why might this best config outperform nearby ones in your grid?\n",
    "\n",
    "> TODO: Your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmei9400b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
