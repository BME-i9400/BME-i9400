{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Homework 3: Receiver Operating Characteristic (ROC) and Precision-Recall Curves\n",
    "\n",
    "\n",
    "**Due date: Wednesday, October 30th 2024, 11:59:59.998 PM EST**"
   ],
   "id": "382a31c653edea95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Objective**: We have seen in class that accuracy is not always an appropriate measure of classifier accuracy (e.g. when the prevalence of one class is close to 1). In this case, we must use an alternative measure of performance. The most common performance metrics in binary classification are:\n",
    "- [**Receiver Operating Characteristic (ROC) Curve**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "- [**Precision-Recall Curve**](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ],
   "id": "bc797dc327acb25a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To be able to compute these, we must first clarify the idea of a *threshold*. In binary classification, a threshold is a value that separates the positive and negative classes. If the predicted probability of the positive class is greater than the threshold, the sample is classified as positive; otherwise, it is classified as negative. \n",
   "id": "e960d202d2dcc901"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We must also define the following terms:\n",
    "- **True Positive Rate (TPR)**: The number of examples correctly predicted as positive divided by the total number of actual positive samples.\n",
    "- **True Negative Rate (TNR)**: The number of examples correctly predicted as negative divided by the total number of actual negative samples.\n",
    "- **False Positive Rate (FPR)**: The number of examples incorrectly predicted as positive divided by the total number of actual negative samples.\n",
    "- **False Negative Rate (FNR)**: The number of examples incorrectly predicted as negative divided by the total number of actual positive samples.\n",
    "\n",
    "- **Precision**: The number of examples correctly predicted as positive divided by the total number of predicted positive samples.\n",
    "- **Recall**: The number of examples correctly predicted as positive divided by the total number of actual positive samples. Note that this is equivalent to the TPR.\n"
   ],
   "id": "4d0b737fa790b625"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To construct an ROC curve, we vary the threshold from 0 to 1 and compute the TPR and FPR at each threshold. The ROC curve is a plot of TPR vs. FPR. The area under the ROC curve (AUC) provides an overall measure of the classifier's performance. AUC ranges from 0 to 1, where 1 indicates a perfect classifier and 0.5 is \"chance\" performance (random guessing).\n",
    "\n",
    "To construct a precision-recall curve, we vary the threshold from 0 to 1 and compute the precision and recall at each threshold. The precision-recall curve is a plot of precision vs. recall. The average precision is the area under the precision-recall curve. Note that chance performance corresponds to an area given by the prevalence of the positive class -- not necessarily 0.5! \n"
   ],
   "id": "d314068caf06abed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this assignment, you will compute these curves from scratch to evaluate the performance of a logistic regression classifier. You will then compare your answers to those obtained using the scikit-learn library. For this homework, we will work with a publicly available [dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) on stroke prediction, where the features characterize the health status of individuals and the target is whether or not they have had a stroke.\n",
    "\n",
    "In the cells below, I have indicated places where code needs to be added with instructions contained in double hashtags (for example ## DO SOMETHING ##). \n"
   ],
   "id": "7f26c29a4c320a43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T20:53:41.006076Z",
     "start_time": "2024-11-05T20:53:40.112643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## THIS CELL IS DONE FOR YOU\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Load the data\n",
    "health_data=pd.read_csv('healthcare-dataset-stroke-data.csv') # csv file must be in the same directory as the notebook\n",
    "\n",
    "# We need to perform some preprocessing to remove missing values and encode categorical variables\n",
    "impute= SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "health_data['bmi']=pd.DataFrame(impute.fit_transform(np.array(health_data['bmi']).reshape(-1, 1)))\n",
    "OH=OneHotEncoder()\n",
    "for col in health_data.select_dtypes(include=['object']):\n",
    "  health_data[col]=OH.fit_transform(health_data[[col]]).toarray()\n",
    "\n",
    "# Separate the features and the target\n",
    "X = health_data.drop('stroke', axis=1).values  # features\n",
    "y = health_data['stroke'].values # targets\n",
    "\n",
    "# Scale the features to have zero mean and unit variance -- this helps gradient descent converge faster\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train logistic regression classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate the predicted _probabilities_ on the test set\n",
    "y_pred_probs = clf.predict_proba(X_test)[:, 1]  # probability of the positive class"
   ],
   "id": "2a54f0a2a8e282e4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T20:53:41.011138Z",
     "start_time": "2024-11-05T20:53:41.008709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ROC Curve construction and AUC calculation\n",
    "\n",
    "# Function to compute TPR, FPR, TNR, FNR\n",
    "def compute_rates(y_true, y_probs, threshold):\n",
    "    \"\"\"\n",
    "    Compute tpr, fpr, tnr, and fnr for a single threshold value\n",
    "    :param y_true: a vector of true labels (0 or 1)\n",
    "    :param y_probs: a vector of predicted probabilities of the positive class\n",
    "    :param threshold: a threshold value (scalar between 0 and 1)\n",
    "    :return: a list whose elements are TPR, FPR, TNR, FNR\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Compute the number of true positives, false positives, true negatives, and false negatives ##\n",
    "\n",
    "    ## Compute the true positive rate (TPR), false positive rate (FPR), true negative rate (TNR), and false negative rate (FNR) ##\n",
    "\n",
    "    ## return the list specified by the docstring ##\n",
    "\n",
    "# Compute ROC Curve\n",
    "def compute_roc_curve(y_true, y_probs):\n",
    "    \"\"\"\n",
    "    Compute TPR and FPR for varying threshold values\n",
    "    :param y_true: \n",
    "    :param y_probs: \n",
    "    :return: a typle of lists, where the first list contains the tpr values and the second list contains the fpr values\n",
    "    \"\"\"\n",
    "    thresholds = np.sort(np.unique(y_probs))\n",
    "    tpr_list, fpr_list = [], []\n",
    "    \n",
    "    ## Populate tpr_list and fpr_list here ##\n",
    "    \n",
    "    return tpr_list, fpr_list\n",
    "\n",
    "## Call the compute_roc_curve function and store the results in tpr and fpr ##\n",
    "\n",
    "## Plot ROC Curve ##\n",
    "\n",
    "## Compute AUC using trapezoidal rule and print out the result ##\n",
    "# Hint: you may have to sort the FPR values before computing the area under the curve\n"
   ],
   "id": "cf29acdebdd82d29",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T20:53:41.077105Z",
     "start_time": "2024-11-05T20:53:41.074516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Precision Recall Curve construction and Average Precision calculation\n",
    "\n",
    "def compute_pr(y_true, y_probs, threshold):\n",
    "    \"\"\"\n",
    "    Compute precision and recall for a single threshold value\n",
    "    :param y_true: vector of true labels (0 or 1)\n",
    "    :param y_probs: vector of predicted probabilities of the positive class\n",
    "    :param threshold: a threshold value (scalar between 0 and 1)\n",
    "    :return: a tuple containing the precision and recall values in first and second positions, respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Compute precision ##\n",
    "    \n",
    "    ## Compute recall ##\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def compute_pr_curve(y_true, y_probs):\n",
    "    \"\"\"\n",
    "    Compute precision and recall for varying threshold values\n",
    "    :param y_true: vector of true labels (0 or 1)\n",
    "    :param y_probs: vector of predicted probabilities of the positive class\n",
    "    :return: a tuple of lists, where the first list contains the precision values and the second list contains the recall values\n",
    "    \"\"\"\n",
    "    thresholds = np.sort(np.unique(y_probs))\n",
    "    precision_list, recall_list = [], []\n",
    "\n",
    "    ## populate precision_list and recall_list here ##\n",
    "\n",
    "    return precision_list, recall_list\n",
    "\n",
    "## Call the compute_pr_curve function and store the results ##\n",
    "\n",
    "## Plot Precision-Recall Curve ##\n",
    "\n",
    "## Compute average precision and report the result ##"
   ],
   "id": "a780b7d1a73adb37",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T20:53:41.085869Z",
     "start_time": "2024-11-05T20:53:41.084363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare your answers with the sklearn implementation\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "## Call the roc_auc_score and average_precision_score functions from scikit-learn your data ##\n",
    "\n",
    "## Print out the results ##"
   ],
   "id": "7be65ec3f061ef7a",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
