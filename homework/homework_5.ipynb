{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Homework 5: Dropout regularization in MLPs and Convolutional Neural Networks\n",
    "\n",
    "**Due date: Wednesday, December 11th 2024, 11:59 PM EST**\n",
    "\n",
    "**Total points: 100**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instructions\n",
    "\n",
    "In this homework you will familiarize yourself with a type of regularization that is commonly used in deep learning models: *dropout*.\n",
    "\n",
    "Dropout is a regularization technique that aims to prevent overfitting by randomly “dropping out” (setting to zero) a subset of neurons and their connections during training. During each training iteration, a randomly selected subset of the neurons is temporarily removed from the network. By removing neurons, dropout forces the network to not rely too heavily on any one neuron, encouraging redundancy and robust feature learning.Note that during model evaluation, dropout is turned off, and all neurons are active, but their outputs are scaled by the dropout rate to maintain consistency with training.\n",
    "\n",
    "Dropout mitigates overfitting by introducing noise into the training process, effectively training many smaller “sub-networks” and averaging their predictions. By training with multiple random sub-networks, the model becomes more robust and less sensitive to specific neurons or weights.\n",
    "\n",
    "In PyTorch, dropout is implemented with the `nn.Dropout` module. The `nn.Dropout` module takes a single argument, `p`, which is the probability of dropping out a neuron. The `p` argument is the probability that a neuron will be zeroed out during training. The `p` argument is typically set to a value between 0.2 and 0.5.\n",
    "\n",
    "To instantiate a dropout layer with a dropout rate of 0.2, you can use the following code:\n",
    "```dropout_layer = nn.Dropout(p=0.2)```\n",
    "\n",
    "To apply dropout to a tensor `x`, you can use the following code:\n",
    "```x = dropout_layer(x)```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset, split into train and test sets, cast to PyTorch tensors, and create data loaders\n",
    "(DONE FOR YOU)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp = np.load('../slides/eeg_alcohol_data.npy', allow_pickle=True)\n",
    "X = tmp.item().get('X')\n",
    "y = tmp.item().get('y')\n",
    "if X.dtype != np.float32:\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "if y.dtype not in [np.int32, np.int64]:\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the CNN model\n",
    "We will work with a model that combines two convolutional layers at the front end with three fully connected layers at the back end. Pay attention to the arguments of the Conv2D and Linear blocks, as it will help your understanding of the model architecture.\n",
    "(DONE FOR YOU)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EEGCNN(nn.Module):\n",
    "    def __init__(self, input_channels, input_timepoints, num_classes):\n",
    "        super(EEGCNN, self).__init__()\n",
    "        self.spatial_conv = nn.Conv2d(1, 8, kernel_size=(input_channels, 1))  # Spatial filtering\n",
    "        self.temporal_conv = nn.Conv2d(8, 16, kernel_size=(1, 10), stride=(1, 2))  # Temporal filtering\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * ((input_timepoints - 10) // 2 + 1), 32) # fully connected layer 1\n",
    "        self.fc2 = nn.Linear(32, 16)  # fully connected layer 2\n",
    "        self.fc3 = nn.Linear(16, num_classes) # fully connected layer 3\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = torch.relu(self.spatial_conv(x))\n",
    "        x = torch.relu(self.temporal_conv(x))\n",
    "        x = self.flatten(x) # convert from 2D to 1D\n",
    "        x = self.fc1(x) # fully connected layer 1\n",
    "        x = self.fc2(x) # fully connected layer 2\n",
    "        x = self.fc3(x) # fully connected layer 3\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate the model with the parameters of our dataset\n",
    "(DONE FOR YOU)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_channels = X_train.shape[1]  # 64 electrodes\n",
    "input_timepoints = X_train.shape[2]  # 256 time samples\n",
    "num_classes = len(torch.unique(y_train))  # Number of unique classes in y\n",
    "model = EEGCNN(input_channels, input_timepoints, num_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the training loop and train for 200 epochs\n",
    "Below is a function that trains the model for a specified number of epochs, reporting the training and test loss after each epoch.\n",
    "(DONE FOR YOU)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_train_loop(model, criterion, optimizer, epochs=200):\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # evaluate test loss\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}\")\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "    return train_losses, test_losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here we train the model for 200 epochs\n",
    "(DONE FOR YOU)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 200\n",
    "train_losses, test_losses = run_train_loop(model, criterion, optimizer, epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Plot the training and test loss (overlaid) on the vertical axis and the epoch number on the horizontal axis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INSERT CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Compute the minimum train and test loss achieved during training and report them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INSERT CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: Modify EEGCNN to add dropout regularization\n",
    "- Modify the cell below to add **three** dropout layers to the EEGCNN model\n",
    "    - The first dropout layer should be added after the flattening layer\n",
    "    - The second dropout layer should be added after the first fully connected layer\n",
    "    - The third dropout layer should be added after the second fully connected layer\n",
    "- The dropout rates of each layer should be the same and should be passed as an argument to the model constructor\n",
    "- The default dropout rate should be 0.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EEGCNNwithDropout(nn.Module):\n",
    "    def __init__(self, input_channels, input_timepoints, num_classes, dropout=0.2):\n",
    "        super(EEGCNNwithDropout, self).__init__()\n",
    "        self.spatial_conv = nn.Conv2d(1, 8, kernel_size=(input_channels, 1))\n",
    "        self.temporal_conv = nn.Conv2d(8, 16, kernel_size=(1, 10), stride=(1, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * ((input_timepoints - 10) // 2 + 1), 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # INSERT CODE BELOW\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = torch.relu(self.spatial_conv(x))\n",
    "        x = torch.relu(self.temporal_conv(x))\n",
    "        x = self.flatten(x)\n",
    "        # INSERT CODE BELOW\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4: Train the EEGCNNwithDropout model with a dropout rate of 0.2 for 200 epochs\n",
    "- Instantiate the model with a dropout rate of 0.2\n",
    "- Train the model for 200 epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INSERT CODE BELOW\n",
    "\n",
    "\n",
    "## DO NOT MODIFY THE 3 LINES BELOW\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_losses, test_losses = run_train_loop(model, criterion, optimizer, 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 5: Plot the training and test loss (overlaid) on the vertical axis and the epoch number on the horizontal axis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 6: Compute the minimum train and test loss achieved during training and report them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 7: Train the EEGCNNwithDropout model with a dropout rate of 0.5 for 200 epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INSERT CODE BELOW\n",
    "\n",
    "## DO NOT MODIFY THE 3 LINES BELOW\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_losses, test_losses = run_train_loop(model, criterion, optimizer, 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 8: Plot the training and test loss (overlaid) on the vertical axis and the epoch number on the horizontal axis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 9: Compute the minimum train and test loss achieved during training and report them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 10: Repeat Tasks 7-9 for a dropout rate of 0.8\n",
    "Report your observations. What do you notice about the train and test loss as the dropout rate increases?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INSERT CODE BELOW"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
