{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minimal OpenRouter LLM Demo\n",
        "\n",
        "This notebook shows **the absolute basics** of calling an LLM via the [OpenRouter](https://openrouter.ai) API using Python.\n",
        "\n",
        "**What you'll need**\n",
        "1. A file named `.env` in the same folder as this notebook with a line like:\n",
        "   ```bash\n",
        "   OPENROUTER_API_KEY=sk-or-v1_...\n",
        "   ```\n",
        "2. An internet connection (calls go to `https://openrouter.ai/api/v1`).\n",
        "\n",
        "We'll keep things minimal: install deps, load the API key from `.env`, make a single non-streaming request, then a streaming request. We also show how to change models and parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (1) Install minimal dependencies (uncomment to run in a fresh environment)\n",
        "%pip install requests python-dotenv --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (2) Load your API key from .env and set up headers\n",
        "import os, requests\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "assert API_KEY, \"Missing OPENROUTER_API_KEY in your .env file\"\n",
        "\n",
        "BASE_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "# Optional headers help attribute your app in OpenRouter's dashboards\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    # Optional but recommended (replace with your course URL or localhost)\n",
        "    \"HTTP-Referer\": \"http://localhost\",  # shown in OpenRouter rankings\n",
        "    \"X-Title\": \"ML Class LLM Demo\",      # a friendly app title\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non‑streaming: single request → single JSON response\n",
        "This mirrors the standard OpenAI-style Chat Completions interface. Change the `model` to any model you have access to in OpenRouter (e.g., `openai/gpt-4o-mini`, `anthropic/claude-3.5-sonnet`, `google/gemini-1.5-pro`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "payload = {\n",
        "    \"model\": \"openai/gpt-4o-mini\",  # pick any model from https://openrouter.ai/models\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain gradient descent in one sentence.\"},\n",
        "    ],\n",
        "    # Optional knobs (common across providers via OpenRouter):\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "\n",
        "resp = requests.post(BASE_URL, headers=HEADERS, json=payload, timeout=60)\n",
        "resp.raise_for_status()\n",
        "data = resp.json()\n",
        "text = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(text)\n",
        "\n",
        "# (Optional) usage metadata\n",
        "data.get(\"usage\", {})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimal helper function\n",
        "A tiny helper to keep classroom examples tidy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(prompt, model=\"openai/gpt-4o-mini\", system=\"You are helpful and concise.\", **params):\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    }\n",
        "    payload.update(params)\n",
        "    r = requests.post(BASE_URL, headers=HEADERS, json=payload, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    out = r.json()\n",
        "    return out[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "print(chat(\"Name three common activation functions.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming (SSE): print tokens as they arrive\n",
        "Set `stream=True` to receive an SSE stream and print chunks as they come in. Ignore comment heartbeats that start with `:`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, sys\n",
        "\n",
        "stream_payload = {\n",
        "    \"model\": \"openai/gpt-4o-mini\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"In two bullet points, what is overfitting?\"},\n",
        "    ],\n",
        "    \"stream\": True,\n",
        "    \"temperature\": 0.2,\n",
        "}\n",
        "\n",
        "with requests.post(BASE_URL, headers=HEADERS, json=stream_payload, stream=True, timeout=300) as r:\n",
        "    r.raise_for_status()\n",
        "    for line in r.iter_lines():\n",
        "        if not line:\n",
        "            continue\n",
        "        # SSE comment/heartbeat lines start with ':' — ignore them\n",
        "        if line.startswith(b\":\"):\n",
        "            continue\n",
        "        if line.startswith(b\"data: \"):\n",
        "            chunk = line[len(b\"data: \"):].decode(\"utf-8\")\n",
        "            if chunk.strip() == \"[DONE]\":\n",
        "                break\n",
        "            event = json.loads(chunk)\n",
        "            delta = event[\"choices\"][0].get(\"delta\", {})\n",
        "            content = delta.get(\"content\")\n",
        "            if content:\n",
        "                print(content, end=\"\", flush=True)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Switching models & parameters\n",
        "Swap the `model` string for any model you have access to (see [openrouter.ai/models](https://openrouter.ai/models)). Common knobs like `temperature`, `max_tokens`, `top_p`, and `frequency_penalty` are supported.\n",
        "\n",
        "> Tip: If a model doesn't support a certain parameter, OpenRouter simply ignores it rather than failing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chat(\n",
        "    \"Write a 1‑sentence analogy for convolution in CNNs.\",\n",
        "    model=\"anthropic/claude-3.5-sonnet\",  # try another provider/model\n",
        "    temperature=0.5,\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**That’s it — minimal and classroom‑friendly.**\n",
        "\n",
        "**Safety note**: Don’t commit your `.env` file to version control. In a shared environment, consider using per‑student keys or a small proxy service so keys aren’t exposed on client machines.\n"
      ]
    }
  ]
}