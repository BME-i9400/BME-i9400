{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal OpenRouter LLM Demo\n",
    "\n",
    "This notebook shows **the absolute basics** of calling an LLM via the [OpenRouter](https://openrouter.ai) API using Python.\n",
    "\n",
    "**What you'll need**\n",
    "1. A file named `.env` in the same folder as this notebook with a line like:\n",
    "   ```bash\n",
    "   OPENROUTER_API_KEY=sk-or-v1_...\n",
    "   ```\n",
    "2. An internet connection (calls go to `https://openrouter.ai/api/v1`).\n",
    "\n",
    "We'll keep things minimal: install deps, load the API key from `.env`, make a single non-streaming request, then a streaming request. We also show how to change models and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfe6d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# (1) Install minimal dependencies (uncomment to run in a fresh environment)\n",
    "%pip install requests python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f599800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Load your API key from .env and set up headers\n",
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "assert API_KEY, \"Missing OPENROUTER_API_KEY in your .env file\"\n",
    "\n",
    "BASE_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "# Optional headers help attribute your app in OpenRouter's dashboards\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # Optional but recommended (replace with your course URL or localhost)\n",
    "    \"HTTP-Referer\": \"http://localhost\",  # shown in OpenRouter rankings\n",
    "    \"X-Title\": \"ML Class LLM Demo\",      # a friendly app title\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561e2a1",
   "metadata": {},
   "source": [
    "## Non‑streaming: single request → single JSON response\n",
    "This mirrors the standard OpenAI-style Chat Completions interface. Change the `model` to any model you have access to in OpenRouter (e.g., `openai/gpt-4o-mini`, `anthropic/claude-3.5-sonnet`, `google/gemini-1.5-pro`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f044fc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent is an optimization algorithm that iteratively adjusts parameters in the direction of the steepest decrease of a loss function to minimize it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 25,\n",
       " 'completion_tokens': 27,\n",
       " 'total_tokens': 52,\n",
       " 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       " 'completion_tokens_details': {'reasoning_tokens': 0}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {\n",
    "    \"model\": \"openai/gpt-4o-mini\",  # pick any model from https://openrouter.ai/models\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain gradient descent in one sentence.\"},\n",
    "    ],\n",
    "    # Optional knobs (common across providers via OpenRouter):\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 256,\n",
    "}\n",
    "\n",
    "resp = requests.post(BASE_URL, headers=HEADERS, json=payload, timeout=60)\n",
    "resp.raise_for_status()\n",
    "data = resp.json()\n",
    "text = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(text)\n",
    "\n",
    "# (Optional) usage metadata\n",
    "data.get(\"usage\", {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8b1da",
   "metadata": {},
   "source": [
    "## Minimal helper function\n",
    "A tiny helper to keep classroom examples tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef128592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three common activation functions are:\n",
      "\n",
      "1. **Sigmoid:** \\( f(x) = \\frac{1}{1 + e^{-x}} \\) - Outputs values between 0 and 1.\n",
      "\n",
      "2. **ReLU (Rectified Linear Unit):** \\( f(x) = \\max(0, x) \\) - Outputs zero for negative values and the input value for positive values.\n",
      "\n",
      "3. **Tanh (Hyperbolic Tangent):** \\( f(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\) - Outputs values between -1 and 1.\n"
     ]
    }
   ],
   "source": [
    "def chat(prompt, model=\"openai/gpt-4o-mini\", system=\"You are helpful and concise.\", **params):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    }\n",
    "    payload.update(params)\n",
    "    r = requests.post(BASE_URL, headers=HEADERS, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    out = r.json()\n",
    "    return out[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "print(chat(\"Name three common activation functions.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092186b",
   "metadata": {},
   "source": [
    "## Streaming (SSE): print tokens as they arrive\n",
    "Set `stream=True` to receive an SSE stream and print chunks as they come in. Ignore comment heartbeats that start with `:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8285d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Overfitting occurs when a machine learning model learns the training data too well, capturing noise and outliers rather than the underlying pattern, leading to poor generalization on unseen data.  \n",
      "- It typically results in high accuracy on the training set but significantly lower accuracy on the validation or test set.\n"
     ]
    }
   ],
   "source": [
    "import json, sys\n",
    "\n",
    "stream_payload = {\n",
    "    \"model\": \"openai/gpt-4o-mini\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"In two bullet points, what is overfitting?\"},\n",
    "    ],\n",
    "    \"stream\": True,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "with requests.post(BASE_URL, headers=HEADERS, json=stream_payload, stream=True, timeout=300) as r:\n",
    "    r.raise_for_status()\n",
    "    for line in r.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        # SSE comment/heartbeat lines start with ':' — ignore them\n",
    "        if line.startswith(b\":\"):\n",
    "            continue\n",
    "        if line.startswith(b\"data: \"):\n",
    "            chunk = line[len(b\"data: \"):].decode(\"utf-8\")\n",
    "            if chunk.strip() == \"[DONE]\":\n",
    "                break\n",
    "            event = json.loads(chunk)\n",
    "            delta = event[\"choices\"][0].get(\"delta\", {})\n",
    "            content = delta.get(\"content\")\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba321e",
   "metadata": {},
   "source": [
    "## Switching models & parameters\n",
    "Swap the `model` string for any model you have access to (see [openrouter.ai/models](https://openrouter.ai/models)). Common knobs like `temperature`, `max_tokens`, `top_p`, and `frequency_penalty` are supported.\n",
    "\n",
    "> Tip: If a model doesn't support a certain parameter, OpenRouter simply ignores it rather than failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcfb89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution in CNNs is like sliding a flashlight (filter) across a dark image, illuminating different patterns one section at a time to detect specific features.\n"
     ]
    }
   ],
   "source": [
    "print(chat(\n",
    "    \"Write a 1‑sentence analogy for convolution in CNNs.\",\n",
    "    model=\"anthropic/claude-3.5-sonnet\",  # try another provider/model\n",
    "    temperature=0.5,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207943b",
   "metadata": {},
   "source": [
    "---\n",
    "**That’s it — minimal and classroom‑friendly.**\n",
    "\n",
    "**Safety note**: Don’t commit your `.env` file to version control. In a shared environment, consider using per‑student keys or a small proxy service so keys aren’t exposed on client machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf19e8",
   "metadata": {},
   "source": [
    "## Applied example: LLM-assisted cohort interpretation\n",
    "\n",
    "In real workflows we often blend numeric outputs with language. Below we build a tiny synthetic cohort, summarize it with Python, then hand the summary to the LLM for a clinical-style briefing. This shows a pattern you can reuse: let code do the number crunching while the model communicates findings for humans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0a5cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"n_patients\": 6,\n",
      "  \"metrics\": {\n",
      "    \"age\": {\n",
      "      \"mean\": 60.2,\n",
      "      \"stdev\": 8.3,\n",
      "      \"min\": 47,\n",
      "      \"max\": 72\n",
      "    },\n",
      "    \"bmi\": {\n",
      "      \"mean\": 29.9,\n",
      "      \"stdev\": 3.6,\n",
      "      \"min\": 24.3,\n",
      "      \"max\": 35.6\n",
      "    },\n",
      "    \"systolic_bp\": {\n",
      "      \"mean\": 140.5,\n",
      "      \"stdev\": 9.4,\n",
      "      \"min\": 126,\n",
      "      \"max\": 155\n",
      "    },\n",
      "    \"hr\": {\n",
      "      \"mean\": 88,\n",
      "      \"stdev\": 9.8,\n",
      "      \"min\": 73,\n",
      "      \"max\": 101\n",
      "    },\n",
      "    \"hba1c\": {\n",
      "      \"mean\": 7.3,\n",
      "      \"stdev\": 1.1,\n",
      "      \"min\": 5.8,\n",
      "      \"max\": 8.9\n",
      "    }\n",
      "  },\n",
      "  \"flags\": {\n",
      "    \"hypertensive_ids\": [\n",
      "      \"P01\",\n",
      "      \"P03\",\n",
      "      \"P05\"\n",
      "    ],\n",
      "    \"tachycardic_ids\": [\n",
      "      \"P03\"\n",
      "    ],\n",
      "    \"poor_glycemic_control_ids\": [\n",
      "      \"P01\",\n",
      "      \"P03\",\n",
      "      \"P05\",\n",
      "      \"P06\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from statistics import mean, pstdev\n",
    "\n",
    "cohort = [\n",
    "    {\"id\": \"P01\", \"age\": 68, \"bmi\": 32.4, \"systolic_bp\": 148, \"hr\": 92, \"hba1c\": 8.4},\n",
    "    {\"id\": \"P02\", \"age\": 54, \"bmi\": 27.1, \"systolic_bp\": 134, \"hr\": 78, \"hba1c\": 6.4},\n",
    "    {\"id\": \"P03\", \"age\": 61, \"bmi\": 29.8, \"systolic_bp\": 142, \"hr\": 101, \"hba1c\": 7.5},\n",
    "    {\"id\": \"P04\", \"age\": 47, \"bmi\": 24.3, \"systolic_bp\": 126, \"hr\": 73, \"hba1c\": 5.8},\n",
    "    {\"id\": \"P05\", \"age\": 72, \"bmi\": 35.6, \"systolic_bp\": 155, \"hr\": 88, \"hba1c\": 8.9},\n",
    "    {\"id\": \"P06\", \"age\": 59, \"bmi\": 30.2, \"systolic_bp\": 138, \"hr\": 96, \"hba1c\": 7.1},\n",
    "]\n",
    "\n",
    "def summarize(field):\n",
    "    values = [p[field] for p in cohort]\n",
    "    return {\n",
    "        \"mean\": round(mean(values), 1),\n",
    "        \"stdev\": round(pstdev(values), 1),\n",
    "        \"min\": min(values),\n",
    "        \"max\": max(values),\n",
    "    }\n",
    "\n",
    "summary = {\n",
    "    \"n_patients\": len(cohort),\n",
    "    \"metrics\": {field: summarize(field) for field in [\"age\", \"bmi\", \"systolic_bp\", \"hr\", \"hba1c\"]},\n",
    "    \"flags\": {\n",
    "        \"hypertensive_ids\": [p[\"id\"] for p in cohort if p[\"systolic_bp\"] >= 140],\n",
    "        \"tachycardic_ids\": [p[\"id\"] for p in cohort if p[\"hr\"] >= 100],\n",
    "        \"poor_glycemic_control_ids\": [p[\"id\"] for p in cohort if p[\"hba1c\"] >= 7.0],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52c29889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reply (not valid JSON):\n",
      "```json\n",
      "{\n",
      "  \"patient_takeaways\": [\n",
      "    \"The cohort consists of 6 patients with a mean age of 60.2 years, indicating a predominantly older population.\",\n",
      "    \"The average BMI is 29.9, suggesting that most patients are overweight.\",\n",
      "    \"Systolic blood pressure averages at 140.5 mmHg, indicating a significant presence of hypertension in the cohort.\",\n",
      "    \"The mean heart rate is 88 bpm, with one patient identified as tachycardic.\",\n",
      "    \"The average HbA1c level is 7.3%, reflecting poor glycemic control in several patients.\"\n",
      "  ],\n",
      "  \"model_watchouts\": [\n",
      "    \"The small cohort size (n=6) may lead to overfitting and unreliable model predictions.\",\n",
      "    \"There is a risk of bias due to the age distribution, as the cohort is predominantly older, which may not generalize to younger populations.\",\n",
      "    \"The presence of multiple health flags (hypertension, tachycardia, poor glycemic control) may introduce confounding factors that complicate model training.\"\n",
      "  ],\n",
      "  \"next_actions\": [\n",
      "    \"Request additional patient data to increase cohort size for more robust model training.\",\n",
      "    \"Analyze the distribution of comorbidities in the cohort to understand potential confounding factors.\",\n",
      "    \"Consider stratifying the cohort by age and BMI to assess the impact of these variables on health outcomes.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = f\"\"\"You are a biomedical machine-learning assistant. Translate the cohort summary into guidance for a clinical data science team.\n",
    "Return JSON with three keys:\n",
    "  - patient_takeaways: bullet-point insights about the cohort.\n",
    "  - model_watchouts: risks or biases to monitor if we train models on this cohort.\n",
    "  - next_actions: suggested follow-up analyses or data you would request.\n",
    "\n",
    "Cohort summary (JSON):\n",
    "{json.dumps(summary, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "raw_reply = chat(\n",
    "    analysis_prompt,\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=400,\n",
    ")\n",
    "\n",
    "try:\n",
    "    briefing = json.loads(raw_reply)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Model reply (not valid JSON):\")\n",
    "    print(raw_reply)\n",
    "else:\n",
    "    for section in (\"patient_takeaways\", \"model_watchouts\", \"next_actions\"):\n",
    "        print(section.replace(\"_\", \" \").title() + \":\")\n",
    "        for bullet in briefing.get(section, []):\n",
    "            print(\" -\", bullet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmei9400b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
