{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Transformers and Large Language Models"
   ],
   "id": "83d3aefbb035da77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Goals of this Lecture\n",
    "\n",
    "1. Understand the fundamentals of the transformer architecture.\n",
    "2. Learn how the attention mechanism works and why it is central to transformers.\n",
    "3. Gain intuition about why transformers are so effective.\n",
    "4. Explore how transformers are used to build large language models through pretraining and fine-tuning."
   ],
   "id": "2189d41bbf4e5ebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Some fantastic resources for understanding the Transformer architecture\n",
    "- Wrapping your head around transformers will take multiple passes. However, it is well worth the effort!\n",
    "- Some resources that I found particularly helpful:\n",
    "    - [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)\n",
    "    - [Understanding self-attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)\n",
    "    - [3 Blue 1 Brown video on transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc) "
   ],
   "id": "c6995b84bfd7d707"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The classic picture of the Transformer architecture\n",
    "- This is the picture that appears in the original paper by Vaswani et al. (2017).\n",
    "    - Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems.\n",
    "- Despite appearing in the paper, it is not a good starting point for understanding transformers.\n",
    "- In fact the original paper is not a good resource for learning the architecture.\n",
    "- In the paper, transformers are mainly posed as a solution to machine translation: converting a sentence in one language to another.\n",
    "<img src=\"transformer.webp\" width=\"600\" height=\"600\">"
   ],
   "id": "f1277f6215329d31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is a Transformer?\n",
    "- *Transformers* are a type of model architecture that has revolutionized natural language processing (NLP).\n",
    "- Key innovation: Self-attention mechanism for processing sequences.\n",
    "    - Replaced previous architectures that included convolutional neural nets (CNNs) models in NLP.\n",
    "- Enabled state-of-the-art performance in:\n",
    "    - Translation\n",
    "    - Summarization\n",
    "    - Question answering\n"
   ],
   "id": "24a9bf924047c759"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer Architecture Overview\n",
    "- Composed of an encoder and decoder\n",
    "    - Encoder: Processes input sequences (i.e., English sentence)\n",
    "\t- Decoder: Generates output sequences (i.e., French translation)\n",
    "- Core components:\n",
    "\t1.\tSelf-Attention: Captures relationships between words in a sequence.\n",
    "\t2.\tFeedforward Layers: Nonlinear transformations for learning representations.\n",
    "\t3.\tPositional Encoding: Adds order information to the sequence.\n",
    "- Stacked layers of attention and feedforward modules allow deep learning."
   ],
   "id": "7bf724ba1536cc23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Attention?\n",
    "\n",
    "- Attention allows the model to focus on relevant parts of the sequence.\n",
    "- Example: In a sentence, “The cat sat on the mat,” the word “cat” is strongly related to “sat.”\n",
    "- Attention weights capture these relationships.\n",
    "- Self-attention captures long-range dependencies better than previous approaches."
   ],
   "id": "416e57e81145b1bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenization, Embedding, Positional Encoding\n",
    "- To start, we convert the input sentence into a sequence of tokens\n",
    "- These tokens are initially represented by integers\n",
    "- We then convert these integers into dense vectors called *embeddings*\n",
    "- To capture the order of the tokens, we add *positional encodings* to the embeddings\n",
    "    - Positional encoding is a sine-cosine function of the position of the token in the sequence\n",
    "    - These sine-cosine functions are literally added to the embeddings\n",
    "<img src=\"embedding.png\" width=\"900\">"
   ],
   "id": "24ca8fa75ab4c633"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Attention Mechanism\n",
    "1.\tInput: A sequence of word embeddings.\n",
    "2.  Query, Key, Value (Q, K, V):\n",
    "    - Represent each word with three vectors.\n",
    "3. Attention Scores:\n",
    "\t    - Compute scores for each word using:\n",
    " \n",
    "___\n",
    "$ \\text{Score}(Q, K) = \\frac{Q \\cdot K^\\top}{\\sqrt{d_k}} $ \n",
    "___\n",
    "        \n",
    "4.  Apply softmax to normalize scores.\n",
    "5. Compute context vector as weighted summation of values:\n",
    "\t    - Combine values (V) weighted by attention scores.\n",
    "**The context vector is the output of the attention mechanism.**\n"
   ],
   "id": "4001365fa5515151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Head Attention\n",
    "-   Splits attention into multiple “heads” for diverse perspectives.\n",
    "-   Each head independently computes attention, focusing on different relationships.\n",
    "-   Outputs are concatenated and linearly transformed.\n",
    "\n",
    "___ \n",
    "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots)W^O$\n",
    "\n",
    "___ \n",
    "<img src=\"QKV.png\" width=\"900\">\n"
   ],
   "id": "2ad591cce9834100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Masked Self-Attention\n",
    "- Attention Score\n",
    "    - The dot product between all query-key pairs is computed and represents how much the quary is related to the key.\n",
    "- Masking\n",
    "    - A \"mask\" is applied to the matrix of attention scores so that the model is not able to see the future words (this would be cheating in the context of language modeling).\n",
    "- Softmax\n",
    "    - Attention scores are converted into probabilities using the softmax function.\n",
    "    - The resulting matrix elements represent how strongly each word relates to the words to its left. \n",
    "<img src=\"attention.png\" width=\"900\" >"
   ],
   "id": "96bc8e232d0b3e90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Output of self-attention block\n",
    "- The (softmax-ed) self-attention scores are multiplied by the value matrix to produce the output of the self-attention block."
   ],
   "id": "7acb439ca185ed30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feedforward Layer\n",
    "- The context vectors from the self-attention block are passed through a feedforward layer.\n",
    "- The task of this layer is to exploit the context of the sentence in order to predict the next word.\n",
    "- In the end, this results in a vector of probabilities, one for each token in the vocabulary.\n",
    "- The token with the highest probability is the predicted next word.\n",
    "<img src=\"mlp.png\" width=\"900\">"
   ],
   "id": "13485c5b835b85a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Are Transformers So Effective?\n",
    "-   Parallelization: Processes entire sequences simultaneously (vs. sequentially in RNNs).\n",
    "-   Scalability: Handles very large datasets and model sizes.\n",
    "-   Representation Power: Captures complex relationships with self-attention.\n",
    "-   Versatility: Works across various tasks with minimal architecture changes."
   ],
   "id": "c78b7a37eea3e8eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How Are LLMs Built?\n",
    "\n",
    "-   Pretraining: train on massive corpora (e.g., books, biomedical papers).\n",
    "-   Pretraining tasks:\n",
    "    -   Masked language modeling (BERT).\n",
    "\t-   Autoregressive prediction (GPT).\n",
    "-   Fine-Tuning:\n",
    "    - Adapt pretrained models to specific tasks with smaller datasets (e.g., question answering)."
   ],
   "id": "60a601dfd8dbc05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformers in Biomedical Engineering\n",
    "-\tLiterature mining: Extract disease-gene associations from PubMed abstracts.\n",
    "-   Clinical notes analysis: Summarize patient records or identify key trends.\n",
    "-\tDrug discovery: Predict drug-target interactions or design new molecules."
   ],
   "id": "36b1296c03f94d75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on demo with the Hugging Face Transformers library",
   "id": "510a91f46ab0202a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:42:43.134549Z",
     "start_time": "2024-12-08T19:37:38.636653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ],
   "id": "28ed73d114ec940c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c81b9305202e4f8ab91a2ede73f89e47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9046be611c32468880569b2e8d854995"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b34a907553d4177b27affe402eb437a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1225f21a92e44ba5b88d64619768f89d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "735fe57a010341188342a50294cab1e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c07fad5144e44c4f95f9fc24da60a416"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3838d4ebd9ee47f9a066536bb9e22292"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to convert any form of non-functional logic - including non-destructive logic such as numbers, functions, sequences, functions, functions with any type - to the highest efficient version. Finally, we will'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "58b7032b11e33eb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
