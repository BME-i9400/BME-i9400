{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2b485b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b18127",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "- Imagine that we would like to build a machine learning model for classifying MRI images of the brain into two categories: images showing a benign tumor versus images showing a malignant tumor.\n",
    "- In this case, the target $y$ is binary, and its value is either 0 or 1: $y \\in \\{0, 1\\}$.\n",
    "- The features $x$ are the pixel values of the MRI image (organized in a vector).\n",
    "- How do we map the features $x$ to the target $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc25f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "- We could use a linear regression model to predict the target $y$.\n",
    "- But this is not an optimal approach: why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c90c473",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression (cont'd)\n",
    "- A better solution is to learn the probability that the target $y$ is 1 given the features $x$\n",
    "---\n",
    "<center> $p(y=1|x) = \\sigma(x)$ </center>\n",
    "\n",
    "---\n",
    "\n",
    "- where $\\sigma(x)$ is the sigmoid function:\n",
    "\n",
    "---\n",
    "<center> $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68666c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sigmoid function\n",
    "- Let's plot the sigmoid function $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "code",
   "id": "72a3ba9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.title('Sigmoid function')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6662a3dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fun fact about the sigmoid function\n",
    "- We know that $p(y=0|x) = 1 - p(y=1|x)$.\n",
    "- Therefore, we can write the probability that the target $y$ is 0 given the features $x$ as follows:\n",
    "- $p(y=0|x) = 1 - p(y=1|x) = 1 - \\sigma(x) = 1 - \\frac{1}{1 + e^{-x}} = \\frac{e^{-x}}{1 + e^{-x}} = \\frac{1}{1 + e^{x}}$\n",
    "- Therefore, we have that \n",
    "---\n",
    "<center> $p(y=0|x) = 1 - \\sigma(x) =  \\sigma(-x)$ </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2271c5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's plot the sigmoid function and its mirror image"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c56c43f",
   "metadata": {},
   "source": [
    "y_0 = sigmoid(-x)\n",
    "plt.plot(x, y, label='sigmoid(x)')\n",
    "plt.plot(x, y_0, label='sigmoid(-x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ce8e3d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sigmoid function (cont'd)\n",
    "- The sigmoid function maps any real number $x$ to the interval $[0, 1]$.\n",
    "- Therefore, we can interpret the output of the sigmoid function as a probability.\n",
    "- For example, if $\\sigma(x) = 0.9$, we can interpret this as a 90% probability that the target $y$ is 1 given the features $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ead72d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression model\n",
    "- The logistic regression model is defined as follows:\n",
    "--- \n",
    "### <center> $p(y=1|x) = \\sigma(w^T x) = \\frac{1}{1 + e^{-w^T x}}$ </center>\n",
    "---\n",
    "- where $w$ is the weight vector and $x$ is the feature vector (includes a constant term).\n",
    "- When $w^T x >> 0$, we have that $p(y=1|x) \\approx 1$.\n",
    "- When $w^T x << 0$, we have that $p(y=1|x) \\approx 0$.\n",
    "- When $w^T x = 0$, we have that $p(y=1|x) = 0.5$.\n",
    "- The linear dot product $w^Tx$ thus captures how likely the target $y$ is 1 given the features $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6549bf2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: benign vs. malignant tumor classification\n",
    "- $y = \\left\\{ \\begin{array}{ll} 0 & \\mbox{benign tumor} \\\\ 1 & \\mbox{malignant tumor} \\end{array} \\right.$\n",
    "- $x = \\left[ \\begin{array}{c} 1 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right] = \\left[ \\begin{array}{c} \\mbox{constant} \\\\ \\mbox{tumor size} \\\\ \\mbox{tumor brightness} \\\\ \\mbox{tumor asymmetry} \\end{array} \\right]$\n",
    "- $w = \\left[ \\begin{array}{c} w_0 \\\\ w_1 \\\\ w_2 \\\\ w_3 \\end{array} \\right]$\n",
    "- $p(y=1|x) =  \\sigma(w^T x) = \\sigma(w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19ae38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective of logistic regression model\n",
    "- Our job is to find the weight vector $w$ that best discriminates benign and malignant tumors.\n",
    "- If the tumor is benign, we would like to have $p(y=1|x) \\approx 0$.\n",
    "- If the tumor is malignant, we would like to have $p(y=1|x) \\approx 1$.\n",
    "- How do we formulate this mathematically?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad566ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss function\n",
    "- Define our estimate of the probability of a malignant tumor as:\n",
    "---\n",
    "<center> $\\hat{y} = \\sigma(w^T x)$. </center>\n",
    "\n",
    "---\n",
    "\n",
    "- Next consider the probability of the correct class:\n",
    "---\n",
    "<center> $ p(y|x) = \\hat{y}^y (1 - \\hat{y})^{1-y}$ </center>\n",
    "\n",
    "Take the case of $y=1$: $p(y|x) = \\hat{y}$\n",
    "\n",
    "Take the case of $y=0$: $p(y|x) = 1- \\hat{y}$\n",
    "\n",
    "---\n",
    "- Notice that when $y=1$, we are left with $\\hat{y}$: this is the model's estimate of the probability of a malignant tumor\n",
    "- When $y=0$, we are left with $1 - \\hat{y}$: this is the model's estimate of the probability of a benign tumor. \n",
    "- Therefore, **we want to maximize this quantity.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5202c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss function (cont'd)\n",
    "- To put this into the framework of loss functions (which we *minimize*), we can take the negative log of the probability:\n",
    "---\n",
    "<center> $L(w) = - \\log(p(y|x)) = - y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})$ </center>\n",
    "\n",
    "---\n",
    "- The log operation is used to make the loss function easy to differentiate.\n",
    "- The negative sign is used to make the loss function a minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4ac28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss function (cont'd)\n",
    "\n",
    "- Across the training set $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)})\\}$, the total loss is:\n",
    "---\n",
    "<center> $L(w) = - \\sum_{i=1}^n y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})$ </center>\n",
    "\n",
    "---\n",
    "- Now we can use gradient descent to minimize this loss function.\n",
    "- It can be shown that the gradient of the loss function is:\n",
    "- $\\nabla_w L(w) = \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}$\n",
    "- or:\n",
    "- $\\nabla_w L(w) = \\sum_{i=1}^n \\left( \\sigma(w^T {x}^{(i)}) - y^{(i)} \\right) x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c83d1e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multinomial logistic regression\n",
    "- The logistic regression model can be extended to the case where the target $y$ can take on more than two values.\n",
    "- In this case, the target $y$ is a categorical variable: $y \\in \\{1, 2, \\ldots, K\\}$, where $K$ is the number of categories.\n",
    "- Before presenting the model, we need to introduce the *softmax* function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d6c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax function\n",
    "- The softmax function is a generalization of the sigmoid function to the case where the target $y$ can take on more than two values.\n",
    "- For a vector $z \\in \\mathbb{R}^K$, the softmax function is defined as follows:\n",
    "---\n",
    "<center> $\\mathrm{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$ </center>\n",
    "\n",
    "---\n",
    "- The softmax function maps a vector $z$ to a probability distribution over $K$ classes\n",
    "    - The values of the softmax function sum to 1, with the larger values being pushed towards 1 (hence the term \"soft\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06614326",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example softmax calculation\n",
    "- Let's demonstrate how the softmax function works with Python code."
   ]
  },
  {
   "cell_type": "code",
   "id": "e4508336",
   "metadata": {},
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "z = np.array([1, -2, 3, 5])\n",
    "softmax(z)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "362630b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: breast cancer dataset\n",
    "- First let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "id": "796465cc",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac909967",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- We will use the breast cancer dataset from scikit-learn.\n",
    "- This dataset contains features computed from digitized images of breast cancer biopsies.\n",
    "- The target is binary: 0 for benign tumors and 1 for malignant tumors.\n",
    "- We will use the logistic regression model to classify the tumors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfd0d1",
   "metadata": {},
   "source": [
    "## Load and inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f58ee14",
   "metadata": {},
   "source": [
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "X.shape, y.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "151e4628",
   "metadata": {},
   "source": [
    "data.keys()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72f0ce4b",
   "metadata": {},
   "source": [
    "## How many features are we predicting from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd35278",
   "metadata": {},
   "source": [
    "## Calculate the prevalence malignant tumors"
   ]
  },
  {
   "cell_type": "code",
   "id": "d132ec5e",
   "metadata": {},
   "source": [
    "np.mean(y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dd025cec",
   "metadata": {},
   "source": [
    "## Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa8b6b99",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7ddae10",
   "metadata": {},
   "source": [
    "## Create and train the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "id": "d72a2f6a",
   "metadata": {},
   "source": [
    "# Create model object\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Fit the model on our training data\n",
    "model.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ead1a773",
   "metadata": {},
   "source": [
    "model.coef_, model.intercept_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "030d577c",
   "metadata": {},
   "source": [
    "## Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "id": "72dd9420",
   "metadata": {},
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# get the model predicted probabilities\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "# print out the first 10 predictions and probabilities\n",
    "y_pred[:10], y_prob[:10]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1979cb6d",
   "metadata": {},
   "source": [
    "## Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "677c6985",
   "metadata": {},
   "source": [
    "acc = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {acc:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f05b2a5b",
   "metadata": {},
   "source": [
    "## Compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3df8c2a",
   "metadata": {},
   "source": [
    "cm = np.zeros((2, 2))\n",
    "for i in range(len(y_test)):\n",
    "    cm[y_test[i], y_pred[i]] += 1\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50dfc1f6",
   "metadata": {},
   "source": [
    "import seaborn\n",
    "seaborn.heatmap(cm, annot=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b43a8bb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multinomial logistic regression model\n",
    "- The multinomial logistic regression model is defined as follows:\n",
    "---\n",
    "### <center> $\\hat{p}(y=k|x) = \\mathrm{softmax}(w_k^T x) = \\frac{e^{w_k^T x}}{\\sum_{j=1}^K e^{w_j^T x}}$ </center>\n",
    "\n",
    "---\n",
    "- where $w_k$ is the weight vector for class $k$.\n",
    "- Note that we have $K$ weight vectors, **one for each class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0fa12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss function for multiple classes\n",
    "- The cross-entropy loss function for the multinomial logistic regression model is:\n",
    "---\n",
    "### <center> $L(w) = - \\sum_{k=1}^K y_k \\log \\hat{p}(y=k|x) $ </center>\n",
    "---\n",
    "- where $y_k$ is the $k$th entry of the one-hot encoded target vector $y$:\n",
    "\n",
    "---\n",
    "<center> $y = \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right]$ </center>\n",
    "\n",
    "---\n",
    "- where the 1 is in the $k$th position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b798dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss function for multiple classes (cont'd)\n",
    "---\n",
    "<center> $L(w) = - \\sum_{k=1}^K \\mathbb{1}(y=k) \\log \\hat{p}(y=k|x) $ </center>\n",
    "\n",
    "---\n",
    "\n",
    "- where $\\mathbb{1}(y=k)$ is the indicator function that is 1 when $y=k$ and 0 otherwise.\n",
    "\n",
    "- Now we can reduce the loss function to:\n",
    "---\n",
    "<center> $L(w) = -\\log \\hat{p}(y=k|x) $ </center>\n",
    "\n",
    "---\n",
    "- because the indicator function will be zero everywhere except for the $k$th value of $\\mathbb{1}(y=k)$.\n",
    "- Notice that the loss is equal to the negative of the log probability.\n",
    "- This is often called the negative log likelihood function. \n",
    "- We can use gradient descent to minimize this loss function on the training set."
   ]
  },
  {
   "cell_type": "code",
   "id": "cce0268d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
