{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BME i9400\n",
    "## Fall 2025\n",
    "### Linear Algebra for Machine Learning - Part II: Eigenanalysis and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today's Problem\n",
    "\n",
    "You are analyzing brain MRI scans from an Alzheimer's disease study. Each scan contains 256×256×180 voxels (≈11.8 million measurements). Your dataset has 500 patients across different disease stages.\n",
    "\n",
    "**The Challenge:**\n",
    "1. How can you find the primary patterns of brain atrophy?\n",
    "2. Can you reduce 11.8 million measurements to just a handful of meaningful features?\n",
    "3. How much information is lost when you compress the data?\n",
    "\n",
    "**Today's goal**: Master eigenanalysis, PCA, and SVD to extract meaningful patterns from high-dimensional biomedical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "1. **Compute** eigenvalues and eigenvectors and interpret their meaning\n",
    "2. **Implement** PCA from scratch using eigendecomposition\n",
    "3. **Apply** SVD for data compression and denoising\n",
    "4. **Evaluate** how many components to retain for analysis\n",
    "5. **Interpret** principal components in clinical context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Lecture: From Covariance to Components (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Eigenvalues and Eigenvectors\n",
    "\n",
    "**Definition:**\n",
    "$$\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$$\n",
    "\n",
    "- $\\mathbf{v}$: eigenvector (direction that doesn't rotate)\n",
    "- $\\lambda$: eigenvalue (scaling factor)\n",
    "\n",
    "**Geometric Intuition:**\n",
    "- Eigenvectors are the \"natural axes\" of your data\n",
    "- Eigenvalues tell you how much variance exists along each axis\n",
    "\n",
    "**For Covariance Matrices:**\n",
    "- Eigenvectors = Principal components (directions of maximum variance)\n",
    "- Eigenvalues = Variance along each principal component\n",
    "- Always orthogonal (perpendicular) for symmetric matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Principal Component Analysis (PCA)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Center data: $\\mathbf{X}_{centered} = \\mathbf{X} - \\bar{\\mathbf{X}}$\n",
    "2. Compute covariance: $\\mathbf{C} = \\frac{1}{n-1}\\mathbf{X}_{centered}^T\\mathbf{X}_{centered}$\n",
    "3. Eigendecomposition: $\\mathbf{C} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^T$\n",
    "4. Sort by eigenvalues (descending)\n",
    "5. Project: $\\mathbf{X}_{new} = \\mathbf{X}_{centered}\\mathbf{V}$\n",
    "\n",
    "**Why PCA?**\n",
    "- Reduces dimensionality while preserving variance\n",
    "- Removes correlations between features\n",
    "- Identifies hidden patterns\n",
    "\n",
    "**Clinical Applications:**\n",
    "- Gene expression: Cell type identification\n",
    "- Neuroimaging: Disease progression patterns\n",
    "- Proteomics: Biomarker discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Singular Value Decomposition (SVD)\n",
    "\n",
    "**Decomposition:**\n",
    "$$\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$$\n",
    "\n",
    "- $\\mathbf{U}$: Left singular vectors (sample patterns)\n",
    "- $\\mathbf{\\Sigma}$: Singular values (importance)\n",
    "- $\\mathbf{V}$: Right singular vectors (feature patterns)\n",
    "\n",
    "**Relationship to PCA:**\n",
    "- Principal components = Right singular vectors ($\\mathbf{V}$)\n",
    "- Eigenvalues = $(\\text{singular values})^2 / (n-1)$\n",
    "- More numerically stable than eigendecomposition\n",
    "\n",
    "**Low-rank Approximation:**\n",
    "$$\\mathbf{X} \\approx \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "Keep only top $k$ components for compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On Lab: Eigenanalysis and Dimensionality Reduction (40 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Work through the following notebook cells, executing the code that has been provided for you, and filling in the code where indicated by `# YOUR CODE HERE`.\n",
    "\n",
    "**Micro-Deliverable**\n",
    "Submit your completed Jupyter notebook in your my-work folder by the end of class."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style and random seed\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Task 1: Understanding Eigendecomposition (10 minutes)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a simple 2D dataset to visualize eigenvectors\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate correlated 2D data\n",
    "mean = [0, 0]\n",
    "cov = [[4, 2.8],\n",
    "       [2.8, 3]]\n",
    "data_2d = np.random.multivariate_normal(mean, cov, 300)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "# Sort by eigenvalues\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov)\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(f\"\\nEigenvector 1 (PC1): {eigenvectors[:, 0]}\")\n",
    "print(f\"Eigenvector 2 (PC2): {eigenvectors[:, 1]}\")\n",
    "print(f\"\\nOrthogonal? (dot product ≈ 0): {np.dot(eigenvectors[:, 0], eigenvectors[:, 1]):.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize eigenvectors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original data with eigenvectors\n",
    "axes[0].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.5, s=20)\n",
    "\n",
    "# Plot eigenvectors scaled by sqrt(eigenvalues)\n",
    "origin = [0, 0]\n",
    "for i in range(2):\n",
    "    axes[0].arrow(origin[0], origin[1],\n",
    "                  eigenvectors[0, i] * np.sqrt(eigenvalues[i]) * 2,\n",
    "                  eigenvectors[1, i] * np.sqrt(eigenvalues[i]) * 2,\n",
    "                  head_width=0.2, head_length=0.2,\n",
    "                  fc='red' if i == 0 else 'blue',\n",
    "                  ec='red' if i == 0 else 'blue',\n",
    "                  linewidth=2,\n",
    "                  label=f'PC{i+1} (λ={eigenvalues[i]:.2f})')\n",
    "\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('Original Data with Eigenvectors')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Project data onto eigenvectors\n",
    "data_projected = data_2d @ eigenvectors\n",
    "\n",
    "axes[1].scatter(data_projected[:, 0], data_projected[:, 1], alpha=0.5, s=20)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('Data in Principal Component Space')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Eigenvectors show the directions of maximum variance\")\n",
    "print(\"- PC1 captures the main trend in the data\")\n",
    "print(\"- PC2 captures the remaining variance orthogonal to PC1\")\n",
    "print(\"- In PC space, features are uncorrelated\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Task 2: Implementing PCA from Scratch (15 minutes)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate gene expression data\n",
    "def generate_gene_expression(n_samples=100, n_genes=500, n_cell_types=3):\n",
    "    \"\"\"Generate synthetic gene expression data with distinct cell types\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Base expression\n",
    "    X = np.random.randn(n_samples, n_genes) * 0.5\n",
    "\n",
    "    # Add cell type-specific signatures\n",
    "    samples_per_type = n_samples // n_cell_types\n",
    "    labels = []\n",
    "\n",
    "    for i in range(n_cell_types):\n",
    "        start = i * samples_per_type\n",
    "        end = min((i + 1) * samples_per_type, n_samples)\n",
    "\n",
    "        # Cell type signature genes\n",
    "        signature_genes = np.random.choice(n_genes, 50, replace=False)\n",
    "        X[start:end, signature_genes] += np.random.randn() * 2 # What is happening in this line of code?\n",
    "\n",
    "        labels.extend([f'Type_{i+1}'] * (end - start))\n",
    "\n",
    "    # Add noise\n",
    "    X += np.random.randn(n_samples, n_genes) * 0.3\n",
    "\n",
    "    return X, labels[:n_samples]\n",
    "\n",
    "# Generate data\n",
    "X_genes, cell_types = generate_gene_expression(n_samples=150, n_genes=1000)\n",
    "print(f\"Gene expression matrix: {X_genes.shape}\")\n",
    "print(f\"  {X_genes.shape[0]} cells × {X_genes.shape[1]} genes\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO 1: Implement PCA from scratch\n",
    "\n",
    "def pca_from_scratch(X, n_components=None):\n",
    "    \"\"\"\n",
    "    Implement PCA using eigendecomposition\n",
    "    \"\"\"\n",
    "    # Step 1: Center the data (subtract out the mean)\n",
    "    # YOUR CODE HERE\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = ...\n",
    "\n",
    "    # Step 2: Compute covariance matrix\n",
    "    # YOUR CODE HERE\n",
    "    n_samples = X.shape[0]\n",
    "    cov_matrix = ...\n",
    "\n",
    "    # Step 3: Eigendecomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # Step 4: Sort by eigenvalues (descending)\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Step 5: Select components\n",
    "    if n_components is not None:\n",
    "        eigenvectors = eigenvectors[:, :n_components]\n",
    "        eigenvalues = eigenvalues[:n_components]\n",
    "\n",
    "    # Step 6: Project data\n",
    "    X_transformed = X_centered @ eigenvectors\n",
    "\n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "    return X_transformed, eigenvectors, eigenvalues, explained_variance_ratio, X_mean\n",
    "\n",
    "# Apply our PCA implementation\n",
    "X_pca_manual, components_manual, eigenvals_manual, var_ratio_manual, _ = pca_from_scratch(X_genes, n_components=10)\n",
    "\n",
    "# Compare with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_std=False)  # Only center, don't scale\n",
    "X_centered_sklearn = scaler.fit_transform(X_genes)\n",
    "\n",
    "pca_sklearn = PCA(n_components=10)\n",
    "X_pca_sklearn = pca_sklearn.fit_transform(X_centered_sklearn)\n",
    "\n",
    "print(\"PCA Implementation Comparison:\")\n",
    "print(f\"\\nExplained variance (first 3 PCs):\")\n",
    "print(f\"  Manual:  {var_ratio_manual[:3]}\")\n",
    "print(f\"  Sklearn: {pca_sklearn.explained_variance_ratio_[:3]}\")\n",
    "\n",
    "# Check if results match (may have sign flip)\n",
    "correlation_pc1 = np.abs(np.corrcoef(X_pca_manual[:, 0], X_pca_sklearn[:, 0])[0, 1])\n",
    "print(f\"\\nPC1 correlation between implementations: {correlation_pc1:.4f}\")\n",
    "print(\"Implementation is correct!\" if correlation_pc1 > 0.999 else \"Check implementation\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize PCA results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Scree plot\n",
    "axes[0, 0].bar(range(1, 11), var_ratio_manual[:10] * 100)\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance (%)')\n",
    "axes[0, 0].set_title('Scree Plot')\n",
    "axes[0, 0].set_xticks(range(1, 11))\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_var = np.cumsum(var_ratio_manual[:10])\n",
    "axes[0, 1].plot(range(1, 11), cumulative_var * 100, 'o-')\n",
    "axes[0, 1].axhline(y=90, color='r', linestyle='--', label='90% threshold')\n",
    "axes[0, 1].set_xlabel('Number of Components')\n",
    "axes[0, 1].set_ylabel('Cumulative Variance (%)')\n",
    "axes[0, 1].set_title('Cumulative Explained Variance')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PC1 vs PC2 scatter\n",
    "colors = {'Type_1': 'red', 'Type_2': 'blue', 'Type_3': 'green'}\n",
    "for cell_type in set(cell_types):\n",
    "    mask = np.array(cell_types) == cell_type\n",
    "    axes[1, 0].scatter(X_pca_manual[mask, 0], X_pca_manual[mask, 1],\n",
    "                      label=cell_type, alpha=0.6, s=30, c=colors[cell_type])\n",
    "\n",
    "axes[1, 0].set_xlabel(f'PC1 ({var_ratio_manual[0]:.1%} var)')\n",
    "axes[1, 0].set_ylabel(f'PC2 ({var_ratio_manual[1]:.1%} var)')\n",
    "axes[1, 0].set_title('Cell Type Clustering')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PC2 vs PC3 scatter\n",
    "for cell_type in set(cell_types):\n",
    "    mask = np.array(cell_types) == cell_type\n",
    "    axes[1, 1].scatter(X_pca_manual[mask, 1], X_pca_manual[mask, 2],\n",
    "                      label=cell_type, alpha=0.6, s=30, c=colors[cell_type])\n",
    "\n",
    "axes[1, 1].set_xlabel(f'PC2 ({var_ratio_manual[1]:.1%} var)')\n",
    "axes[1, 1].set_ylabel(f'PC3 ({var_ratio_manual[2]:.1%} var)')\n",
    "axes[1, 1].set_title('Alternative View')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_components_90 = np.argmax(cumulative_var >= 0.9) + 1\n",
    "print(f\"\\nComponents needed for 90% variance: {n_components_90}\")\n",
    "print(f\"Dimensionality reduction: {X_genes.shape[1]} → {n_components_90} features\")\n",
    "print(f\"Compression ratio: {X_genes.shape[1] / n_components_90:.1f}:1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Task 3: SVD for Medical Image Compression (15 minutes)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate synthetic brain scan data\n",
    "def generate_brain_scans(n_scans=50, image_size=64):\n",
    "    \"\"\"Generate synthetic brain scan data with disease progression\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    x, y = np.meshgrid(np.linspace(-1, 1, image_size),\n",
    "                       np.linspace(-1, 1, image_size))\n",
    "\n",
    "    # Define brain regions (simplified)\n",
    "    # Central ventricles\n",
    "    ventricles = np.exp(-(x**2 + y**2) / 0.1)\n",
    "\n",
    "    # Cortical regions\n",
    "    cortex = np.exp(-((x**2 + y**2) - 0.5)**2 / 0.1)\n",
    "\n",
    "    # Subcortical structures\n",
    "    subcortical = (np.exp(-((x-0.3)**2 + y**2) / 0.05) +\n",
    "                   np.exp(-((x+0.3)**2 + y**2) / 0.05))\n",
    "\n",
    "    scans = []\n",
    "    disease_stages = []\n",
    "\n",
    "    for i in range(n_scans):\n",
    "        # Disease progression (0=healthy, 1=severe)\n",
    "        stage = i / n_scans\n",
    "\n",
    "        # Simulate atrophy: ventricles enlarge, cortex shrinks\n",
    "        scan = (1 - 0.3 * stage) * cortex + \\\n",
    "               (1 + 0.5 * stage) * ventricles + \\\n",
    "               (1 - 0.2 * stage) * subcortical\n",
    "\n",
    "        # Add noise\n",
    "        scan += np.random.normal(0, 0.1, scan.shape)\n",
    "        scan = np.clip(scan, 0, None)\n",
    "\n",
    "        scans.append(scan.flatten())\n",
    "        disease_stages.append(stage)\n",
    "\n",
    "    return np.array(scans), np.array(disease_stages), image_size\n",
    "\n",
    "# Generate data\n",
    "X_scans, disease_stages, img_size = generate_brain_scans(n_scans=100)\n",
    "print(f\"Brain scan data: {X_scans.shape}\")\n",
    "print(f\"  {X_scans.shape[0]} scans × {X_scans.shape[1]} pixels\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO 2: Perform SVD on brain scans\n",
    "\n",
    "# Center the data\n",
    "X_scans_mean = np.mean(X_scans, axis=0)\n",
    "X_scans_centered = X_scans - X_scans_mean\n",
    "\n",
    "# YOUR CODE HERE: Perform SVD\n",
    "# Hint: use np.linalg.svd with full_matrices=False\n",
    "U, S, Vt = ...\n",
    "\n",
    "print(\"SVD Decomposition:\")\n",
    "print(f\"  U (scan coefficients): {U.shape}\")\n",
    "print(f\"  S (singular values): {S.shape}\")\n",
    "print(f\"  Vt (image components): {Vt.shape}\")\n",
    "\n",
    "# Calculate variance explained\n",
    "variance_explained = (S**2) / np.sum(S**2)\n",
    "cumulative_variance_svd = np.cumsum(variance_explained)\n",
    "\n",
    "# Visualize singular value decay\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Singular values\n",
    "axes[0].semilogy(S[:30], 'o-')\n",
    "axes[0].set_xlabel('Component')\n",
    "axes[0].set_ylabel('Singular Value')\n",
    "axes[0].set_title('Singular Value Spectrum')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance per component\n",
    "axes[1].bar(range(1, 16), variance_explained[:15] * 100)\n",
    "axes[1].set_xlabel('Component')\n",
    "axes[1].set_ylabel('Variance Explained (%)')\n",
    "axes[1].set_title('Individual Component Variance')\n",
    "\n",
    "# Cumulative variance\n",
    "axes[2].plot(range(1, 31), cumulative_variance_svd[:30] * 100, 'o-')\n",
    "axes[2].axhline(y=90, color='r', linestyle='--', label='90%')\n",
    "axes[2].axhline(y=95, color='orange', linestyle='--', label='95%')\n",
    "axes[2].axhline(y=99, color='green', linestyle='--', label='99%')\n",
    "axes[2].set_xlabel('Number of Components')\n",
    "axes[2].set_ylabel('Cumulative Variance (%)')\n",
    "axes[2].set_title('Cumulative Variance')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report compression potential\n",
    "for threshold in [0.90, 0.95, 0.99]:\n",
    "    n_comp = np.argmax(cumulative_variance_svd >= threshold) + 1\n",
    "    compression = X_scans.shape[1] / n_comp\n",
    "    print(f\"{threshold:.0%} variance: {n_comp:3d} components, {compression:5.1f}:1 compression\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize principal brain components (\"eigenbrains\")\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "\n",
    "# Show mean brain\n",
    "axes[0, 0].imshow(X_scans_mean.reshape(img_size, img_size), cmap='gray')\n",
    "axes[0, 0].set_title('Mean Brain')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show first 11 components\n",
    "for i in range(11):\n",
    "    row = (i + 1) // 4\n",
    "    col = (i + 1) % 4\n",
    "\n",
    "    component = Vt[i].reshape(img_size, img_size)\n",
    "    axes[row, col].imshow(component, cmap='RdBu_r')\n",
    "    axes[row, col].set_title(f'PC{i+1} ({variance_explained[i]:.1%})')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Principal Components of Brain Scans', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation of Components:\")\n",
    "print(\"  PC1: Overall brain size/intensity variation\")\n",
    "print(\"  PC2-3: Ventricular enlargement (atrophy pattern)\")\n",
    "print(\"  PC4-5: Cortical thickness variations\")\n",
    "print(\"  Later PCs: Finer anatomical details and noise\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO 3: Image reconstruction with different numbers of components\n",
    "\n",
    "# Select test scans at different disease stages\n",
    "test_indices = [0, 25, 50, 75, 99]  # Healthy to severe\n",
    "n_components_list = [1, 5, 10, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(len(test_indices), len(n_components_list) + 1,\n",
    "                        figsize=(15, 12))\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    # Original scan\n",
    "    original = X_scans[idx].reshape(img_size, img_size)\n",
    "    axes[i, 0].imshow(original, cmap='gray', vmin=0, vmax=2)\n",
    "    axes[i, 0].set_title(f'Original\\nStage: {disease_stages[idx]:.1f}')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Reconstructions\n",
    "    for j, n_comp in enumerate(n_components_list):\n",
    "        # YOUR CODE HERE: Reconstruct using n_comp components\n",
    "        X_reconstructed = ...\n",
    "        X_reconstructed += X_scans_mean\n",
    "\n",
    "        reconstructed = X_reconstructed[idx].reshape(img_size, img_size)\n",
    "\n",
    "        axes[i, j+1].imshow(reconstructed, cmap='gray', vmin=0, vmax=2)\n",
    "        axes[i, j+1].set_title(f'{n_comp} PCs\\n({cumulative_variance_svd[n_comp-1]:.1%})')\n",
    "        axes[i, j+1].axis('off')\n",
    "\n",
    "plt.suptitle('Brain Scan Reconstruction Across Disease Stages', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "print(\"Reconstruction Mean Squared Error:\")\n",
    "print(\"Components:\", end=\"\")\n",
    "for n_comp in n_components_list:\n",
    "    print(f\"{n_comp:8d}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for idx in test_indices:\n",
    "    print(f\"Stage {disease_stages[idx]:.1f}: \", end=\"\")\n",
    "    for n_comp in n_components_list:\n",
    "        X_reconstructed = U[:, :n_comp] @ np.diag(S[:n_comp]) @ Vt[:n_comp, :]\n",
    "        error = np.mean((X_scans_centered[idx] - X_reconstructed[idx])**2)\n",
    "        print(f\"{error:8.4f}\", end=\"\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Eigenanalysis reveals the natural structure of data\n",
    "- PCA reduces dimensionality while preserving variance\n",
    "- SVD provides stable, efficient matrix factorization\n",
    "- 90% variance often captured with 10-20% of original dimensions\n",
    "- Principal components often have biological interpretation\n",
    "\n",
    "**Clinical Applications:**\n",
    "- Disease subtype discovery\n",
    "- Image compression and denoising\n",
    "- Biomarker panel reduction\n",
    "- Patient stratification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
