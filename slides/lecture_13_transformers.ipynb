{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Transformers and Large Language Models"
   ],
   "id": "83d3aefbb035da77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Goals of this Lecture\n",
    "\n",
    "1. Understand the fundamentals of the transformer architecture.\n",
    "2. Learn how the attention mechanism works and why it is central to transformers.\n",
    "3. Gain intuition about why transformers are so effective.\n",
    "4. Explore how transformers are used to build large language models through pretraining and fine-tuning."
   ],
   "id": "2189d41bbf4e5ebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Some fantastic resources for understanding the Transformer architecture\n",
    "- Wrapping your head around transformers will take multiple passes. However, it is well worth the effort!\n",
    "- Some resources that I found particularly helpful:\n",
    "    - [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)\n",
    "    - [Understanding self-attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)\n",
    "    - [3 Blue 1 Brown video on transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc) "
   ],
   "id": "c6995b84bfd7d707"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The classic picture of the Transformer architecture\n",
    "- This is the picture that appears in the original paper by Vaswani et al. (2017).\n",
    "    - Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems.\n",
    "- Despite appearing in the paper, it is not a good starting point for understanding transformers.\n",
    "- In fact the original paper is not a good resource for learning the architecture.\n",
    "- In the paper, transformers are mainly posed as a solution to machine translation: converting a sentence in one language to another.\n",
    "<img src=\"transformer.webp\" width=\"600\" height=\"600\">"
   ],
   "id": "f1277f6215329d31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is a Transformer?\n",
    "- *Transformers* are a type of model architecture that has revolutionized natural language processing (NLP).\n",
    "- Key innovation: Self-attention mechanism for processing sequences.\n",
    "    - Replaced previous architectures that included convolutional neural nets (CNNs) models in NLP.\n",
    "- Enabled state-of-the-art performance in:\n",
    "    - Translation\n",
    "    - Summarization\n",
    "    - Question answering\n"
   ],
   "id": "24a9bf924047c759"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer Architecture Overview\n",
    "- Composed of an encoder and decoder\n",
    "    - Encoder: Processes input sequences (i.e., English sentence)\n",
    "\t- Decoder: Generates output sequences (i.e., French translation)\n",
    "- Core components:\n",
    "\t1.\tSelf-Attention: Captures relationships between words in a sequence.\n",
    "\t2.\tFeedforward Layers: Nonlinear transformations for learning representations.\n",
    "\t3.\tPositional Encoding: Adds order information to the sequence.\n",
    "- Stacked layers of attention and feedforward modules allow deep learning."
   ],
   "id": "7bf724ba1536cc23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Attention?\n",
    "\n",
    "- Attention allows the model to focus on relevant parts of the sequence.\n",
    "- Example: In a sentence, ‚ÄúThe cat sat on the mat,‚Äù the word ‚Äúcat‚Äù is strongly related to ‚Äúsat.‚Äù\n",
    "- Attention weights capture these relationships.\n",
    "- Self-attention captures long-range dependencies better than previous approaches."
   ],
   "id": "416e57e81145b1bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenization, Embedding, Positional Encoding\n",
    "- To start, we convert the input sentence into a sequence of tokens\n",
    "- These tokens are initially represented by integers\n",
    "- We then convert these integers into dense vectors called *embeddings*\n",
    "- To capture the order of the tokens, we add *positional encodings* to the embeddings\n",
    "    - Positional encoding is a sine-cosine function of the position of the token in the sequence\n",
    "    - These sine-cosine functions are literally added to the embeddings\n",
    "<img src=\"embedding.png\" width=\"900\">"
   ],
   "id": "24ca8fa75ab4c633"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Attention Mechanism\n",
    "1.\tInput: A sequence of word embeddings.\n",
    "2.  Query, Key, Value (Q, K, V):\n",
    "    - Represent each word with three vectors.\n",
    "3. Attention Scores:\n",
    "\t    - Compute scores for each word using:\n",
    " \n",
    "___\n",
    "$ \\text{Score}(Q, K) = \\frac{Q \\cdot K^\\top}{\\sqrt{d_k}} $ \n",
    "___\n",
    "        \n",
    "4.  Apply softmax to normalize scores.\n",
    "5. Compute context vector as weighted summation of values:\n",
    "\t    - Combine values (V) weighted by attention scores.\n",
    "**The context vector is the output of the attention mechanism.**\n"
   ],
   "id": "4001365fa5515151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Head Attention\n",
    "-   Splits attention into multiple ‚Äúheads‚Äù for diverse perspectives.\n",
    "-   Each head independently computes attention, focusing on different relationships.\n",
    "-   Outputs are concatenated and linearly transformed.\n",
    "\n",
    "___ \n",
    "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots)W^O$\n",
    "\n",
    "___ \n",
    "<img src=\"QKV.png\" width=\"900\">\n"
   ],
   "id": "2ad591cce9834100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Masked Self-Attention\n",
    "- Attention Score\n",
    "    - The dot product between all query-key pairs is computed and represents how much the quary is related to the key.\n",
    "- Masking\n",
    "    - A \"mask\" is applied to the matrix of attention scores so that the model is not able to see the future words (this would be cheating in the context of language modeling).\n",
    "- Softmax\n",
    "    - Attention scores are converted into probabilities using the softmax function.\n",
    "    - The resulting matrix elements represent how strongly each word relates to the words to its left. \n",
    "<img src=\"attention.png\" width=\"900\" >"
   ],
   "id": "96bc8e232d0b3e90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Output of self-attention block\n",
    "- The (softmax-ed) self-attention scores are multiplied by the value matrix to produce the output of the self-attention block."
   ],
   "id": "7acb439ca185ed30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feedforward Layer\n",
    "- The context vectors from the self-attention block are passed through a feedforward layer.\n",
    "- The task of this layer is to exploit the context of the sentence in order to predict the next word.\n",
    "- In the end, this results in a vector of probabilities, one for each token in the vocabulary.\n",
    "- The token with the highest probability is the predicted next word.\n",
    "<img src=\"mlp.png\" width=\"900\">"
   ],
   "id": "13485c5b835b85a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Are Transformers So Effective?\n",
    "-   Parallelization: Processes entire sequences simultaneously (vs. sequentially in RNNs).\n",
    "-   Scalability: Handles very large datasets and model sizes.\n",
    "-   Representation Power: Captures complex relationships with self-attention.\n",
    "-   Versatility: Works across various tasks with minimal architecture changes."
   ],
   "id": "c78b7a37eea3e8eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How Are LLMs Built?\n",
    "\n",
    "-   Pretraining: train on massive corpora (e.g., books, biomedical papers).\n",
    "-   Pretraining tasks:\n",
    "    -   Masked language modeling (BERT).\n",
    "\t-   Autoregressive prediction (GPT).\n",
    "-   Fine-Tuning:\n",
    "    - Adapt pretrained models to specific tasks with smaller datasets (e.g., question answering)."
   ],
   "id": "60a601dfd8dbc05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformers in Biomedical Engineering\n",
    "-\tLiterature mining: Extract disease-gene associations from PubMed abstracts.\n",
    "-   Clinical notes analysis: Summarize patient records or identify key trends.\n",
    "-\tDrug discovery: Predict drug-target interactions or design new molecules."
   ],
   "id": "36b1296c03f94d75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hands on demo with the Hugging Face Transformers library",
   "id": "510a91f46ab0202a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:30:29.386799Z",
     "start_time": "2024-12-09T20:30:26.527166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load a pretrained masked language model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Encode a sentence with a masked token\n",
    "text = \"The heart is a [MASK] organ.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Display the token IDs\n",
    "print(\"Input IDs:\", inputs.input_ids)"
   ],
   "id": "a6208a0f33a9ed25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 1996, 2540, 2003, 1037,  103, 5812, 1012,  102]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:30:36.355550Z",
     "start_time": "2024-12-09T20:30:35.265093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate the mode's prediction for the masked token\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get predicted token for the masked position\n",
    "predictions = outputs.logits\n",
    "mask_index = inputs.input_ids[0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = predictions[0, mask_index].argmax(dim=-1).item()\n",
    "print(\"Predicted Token:\", tokenizer.decode([predicted_token_id]))"
   ],
   "id": "85dc935a779fe3df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Token: cardiac\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:30:51.645190Z",
     "start_time": "2024-12-09T20:30:51.640283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the top 5 predictions\n",
    "top_k = 5\n",
    "top_predictions = predictions[0, mask_index].topk(top_k).indices\n",
    "top_predictions = [tokenizer.decode([token_id]) for token_id in top_predictions]\n",
    "print(\"Top Predictions:\", top_predictions)"
   ],
   "id": "d49a4edb86d34b19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Predictions: ['cardiac', 'large', 'major', 'living', 'mechanical']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finally, what you've all been waiting for",
   "id": "98b38fe367de7e4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Install the openai API\n",
    "```! pip install openai```"
   ],
   "id": "a53ee170ac4a8804"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the API key",
   "id": "a760e66a8e86b507"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:27:01.080309Z",
     "start_time": "2024-12-09T20:27:01.074348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(dotenv_path=\"/Users/jacekdmochowski/.env\")  # Adjust the path if your .env file is elsewhere\n",
    "\n",
    "# Access the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "id": "58b7032b11e33eb7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Talk to the GPT-4o model",
   "id": "c2f7750314de70e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:29:13.984243Z",
     "start_time": "2024-12-09T20:29:06.040932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert forecaster of the future.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What will a second Trump presidency mean for the future of biomedical engineering?\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ],
   "id": "3f2f3a83639c21d3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:29:18.731741Z",
     "start_time": "2024-12-09T20:29:18.727955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = completion.choices[0].message.content\n",
    "# render with line breaks\n",
    "print(response.replace(\"\\n\", \"\\n\"))"
   ],
   "id": "321f06a18d452249",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an expert in forecasting, I can provide some insights based on trends and historical data up to October 2023. However, predicting the future with absolute certainty is impossible. Should a second Trump presidency occur, several factors could potentially influence the field of biomedical engineering:\n",
      "\n",
      "1. **Regulatory Environment**: Historically, Republican administrations, including the first Trump presidency, focused on deregulation. A second Trump presidency might accelerate the approval process for biomedical innovations, possibly favoring industry growth but raising concerns about safety and efficacy standards.\n",
      "\n",
      "2. **Funding and Investment**: The level of federal funding for scientific research, including biomedical engineering, could fluctuate. While the Trump administration previously proposed cuts to agencies like the NIH, budget outcomes depended heavily on congressional negotiations. Public-private partnerships might be encouraged to fill any gaps.\n",
      "\n",
      "3. **Healthcare Policy**: Changes to healthcare policy, such as modifications or repeal of the Affordable Care Act, could impact biomedical engineering. Innovations might focus on cost-reduction technologies, personalized medicine, and telehealth to fit new healthcare models.\n",
      "\n",
      "4. **Immigration Policies**: Biomedical engineering is a field that greatly benefits from international talent. Stricter immigration policies may affect the workforce, potentially limiting the pool of skilled professionals and researchers.\n",
      "\n",
      "5. **Trade Policies**: Tariff policies and international trade agreements might influence the cost and availability of materials essential for biomedical innovation, thereby impacting the field‚Äôs growth.\n",
      "\n",
      "6. **Focus on Domestic Manufacturing**: A renewed focus on domestic manufacturing could encourage the development of biomedical technologies within the U.S., potentially leading to more innovation and job creation in the sector.\n",
      "\n",
      "Overall, a second Trump presidency could lead to a complex mix of opportunities and challenges for the field of biomedical engineering, shaped by domestic policy decisions and global economic conditions.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "611fd5b44d088e36"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
