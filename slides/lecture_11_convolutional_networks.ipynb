{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392eae2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BME i9400\n",
    "## Fall 2025\n",
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25280b31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "- In previous lectures, we covered vanilla neural networks: multilayer perceptrons\n",
    "- The building blocks of MLPs are *fully connected layers*\n",
    "- Before introducing convolutional networks, let's define a fully connected layer, as it will be used later in the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56d579",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fully connected layer\n",
    "- A fully connected layer is a layer in a neural network where each neuron is connected to every neuron in the previous layer\n",
    "- The output of a fully connected layer is computed as:\n",
    "---\n",
    "$y = \\sigma(Wx + b)$\n",
    "\n",
    "---\n",
    "where:\n",
    "- $y$ is the output of the layer (generally a vector)\n",
    "- $\\sigma$ is the activation function\n",
    "- $W$ is the weight matrix\n",
    "- $x$ is the input vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bd171",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fully connected layer\n",
    "<img src=\"img/dense2.png\" alt=\"ANN Diagram\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d5457",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "- Convolutional Neural Networks (CNNs) are a class of neural networks that are particularly well-suited for processing data with *spatial* or *temporal* structure\n",
    "- Examples in biomedical engineering:\n",
    "    - Medical image analysis\n",
    "    - ECG/EEG signal processing\n",
    "    - Protein structure prediction\n",
    "    - Drug discovery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be70c4cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution in 1-D\n",
    "- As the name suggests, the fundamental operation in CNNs is *convolution*\n",
    "- Convolution is a mathematical operation between two signals\n",
    "    - One of these signals may be viewed as the input, and the second as a filter\n",
    "    - NB: the \"filter\" is often referred to as a \"kernel\" in the context of CNNs\n",
    "    - Convolving the input with the filter produces an output signal (a third signal)\n",
    "- In 1-dimension, convolutions are often thought of as a temporal filtering operation:\n",
    "---\n",
    "$y(t) = (x * w)(t) = \\sum_{a=-\\infty}^{\\infty} x(a)w(t-a)$\n",
    "\n",
    "---\n",
    "where $x$ is the input signal, $w$ is the filter, and $y$ is the output signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb7731",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution in 2-D\n",
    "- In 2 dimensions, the convolution operation is defined as:\n",
    "---\n",
    "$y(i,j) = (x * w)(i,j) = \\sum_m \\sum_n x(m,n)w(i-m,j-n)$\n",
    "\n",
    "---\n",
    "where $x$ is the 2-D input image (a 2-D image), $w$ is the 2-D filter, and $y$ is the output image.\n",
    "- Convolution in 2-D may be thought of as sliding the filter over the input image, and computing the dot product at each location\n",
    "- The output image is typically smaller than the input image\n",
    "- One example of a filter is the Sobel filter, which is used for edge detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f88dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstrating convolution in 1-D\n",
    "- Without any further ado, let's demonstrate convolution in 1-D (super exciting, I know!)\n",
    "- Good news: ```numpy``` provides a function for convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T20:46:15.719464Z",
     "start_time": "2024-11-11T20:46:15.223597Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1D Convolution Example\n",
    "def conv1d(signal, kernel):\n",
    "    return np.convolve(signal, kernel, mode='valid')\n",
    "\n",
    "# Create a sample signal and kernel\n",
    "t = np.linspace(0, 10, 1000)\n",
    "signal = np.sin(2*np.pi*t) + np.random.normal(0, 0.1, len(t))\n",
    "kernel = np.ones(50)/50  # Moving average filter\n",
    "\n",
    "# Apply convolution\n",
    "filtered_signal = conv1d(signal, kernel)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t[:len(filtered_signal)], filtered_signal, label='Filtered')\n",
    "plt.plot(t[:len(filtered_signal)], signal[:len(filtered_signal)], alpha=0.5, label='Original')\n",
    "plt.title('1D Convolution Example')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e5c68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstrating 2-D convolutions\n",
    "- To demonstrate convolution in 2-D, we will use the ```scipy``` package to create a simple image and apply a 2-D convolution\n",
    "- We will use a simple edge detection kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d32174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T20:52:07.670387Z",
     "start_time": "2024-11-11T20:52:06.541791Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "# Create a sample image\n",
    "image = np.zeros((10, 10))\n",
    "image[4:7, 4:7] = 1\n",
    "\n",
    "# Create an edge detection kernel\n",
    "kernel = np.array([[-1, -1, -1],\n",
    "                  [-1,  8, -1],\n",
    "                  [-1, -1, -1]])\n",
    "\n",
    "# Apply 2D convolution\n",
    "conv_result = signal.convolve2d(image, kernel, mode='valid')\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax1.imshow(image, cmap='hot', vmin=-1, vmax=1)\n",
    "ax1.set_title('Original Image')\n",
    "plt.colorbar(ax1.imshow(image, cmap='hot', vmin=-1, vmax=1), ax=ax1)\n",
    "ax2.imshow(conv_result, cmap='hot', vmin=-1, vmax=1)\n",
    "ax2.set_title('After Convolution')\n",
    "plt.colorbar(ax2.imshow(conv_result, cmap='hot', vmin=-1, vmax=1), ax=ax2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the original image and the filtered image BOTH USING SEABORN HEATMPA AND BOTH USING ANNOTATIONS\n",
    "# DO NOT MESS THIS UP\n",
    "import seaborn as sns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax1.imshow(image, cmap='hot', vmin=-1, vmax=1)\n",
    "ax1.set_title('Original Image')\n",
    "sns.heatmap(image, cmap='hot', vmin=-1, vmax=1, annot=True, fmt='.1f', ax=ax1)\n",
    "ax2.imshow(conv_result, cmap='hot', vmin=-1, vmax=1)\n",
    "ax2.set_title('After Convolution')\n",
    "sns.heatmap(conv_result, cmap='hot', vmin=-1, vmax=1, annot=True, fmt='.1f', ax=ax2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab5a67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Use Convolutions in Neural Networks?\n",
    "1. Weight Sharing\n",
    "   - Same kernel is applied across the entire input\n",
    "   - Reduces number of parameters\n",
    "   - Translation invariance\n",
    "\n",
    "2. Local Features\n",
    "   - Each output value depends only on nearby input values\n",
    "   - Captures spatial relationships\n",
    "   - Hierarchical feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77625b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1D Convolutional Layers\n",
    "- Used for processing *sequential* data\n",
    "- The filter slides across the input, computing an output at each location (see below)\n",
    "- Examples in biomedical engineering:\n",
    "    - ECG signals\n",
    "    - EEG recordings\n",
    "    - Blood pressure time series\n",
    "\n",
    "## 1D Convolutional Layer\n",
    "<img src=\"img/conv1d.jpeg\" alt=\"1D Convolution\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4661000",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D Convolutional Layers\n",
    "- Used for processing image data\n",
    "- The filter slides across the input image (moves left to right, as well as top to bottom), computing an output at each location\n",
    "- Examples in biomedical engineering:\n",
    "    - X-ray images\n",
    "    - MRI scans\n",
    "    - Histology slides\n",
    "    - Microscopy images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864c45b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D Convolutional Layer\n",
    "<img src=\"img/conv2d.jpeg\" alt=\"2D Convolution\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baba3ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layers\n",
    "- Convolutions are almost always paired with *pooling*\n",
    "    - Pooling means that we reduce the value of a region of the input to a single value\n",
    "- Types:\n",
    "    - Max pooling (most common): take the maximum value in the region\n",
    "    - Average pooling: take the average value in the region\n",
    "- Benefits:\n",
    "    - Reduces computation\n",
    "    - Provides some translation invariance\n",
    "    - Helps prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd46914",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Max pooling example\n",
    "<img src=\"img/pooling.jpeg\" alt=\"Max Pooling\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101443f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Striding\n",
    "- Controls how the kernel moves across the input\n",
    "- A stride of 1 means that the kernel moves one pixel at a time\n",
    "- Larger stride = reduced output size\n",
    "- Can be used instead of or in addition to pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82602a",
   "metadata": {},
   "source": [
    "## Note to self: in the image below, the Stride=1 panel assumes a 2x2 kernel, NOT 3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a2113",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Striding\n",
    "<img src=\"img/strides.jpeg\" alt=\"Striding\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90d59b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of CNNs in Biomedical Engineering\n",
    "1. 1D CNNs:\n",
    "   - ECG classification\n",
    "   - Sleep stage scoring\n",
    "   - Seizure detection\n",
    "\n",
    "2. 2D CNNs:\n",
    "   - Medical image segmentation\n",
    "   - Disease classification\n",
    "   - Cell detection\n",
    "\n",
    "3. 3D CNNs:\n",
    "   - Volumetric medical imaging (CT, MRI)\n",
    "   - Drug-protein interaction prediction\n",
    "   - Motion analysis in medical videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b405b3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstrating 2-D CNNs in PyTorch\n",
    "- Let's create a simple CNN model in PyTorch\n",
    "- We will work with the popular MNIST dataset of handwritten digits\n",
    "- The goal of the model is to classify the digits into one of 10 classes (0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ebf1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T21:23:13.256524Z",
     "start_time": "2024-11-11T21:23:11.625385Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the CNN\n",
    "class SimpleCNN(nn.Module): # defining a model that we will call SimpleCNN -- it will inherit from nn.Module\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=(5,5)) # 1 input \"channel\", 10 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=(5,5)) # 10 input channels, 20 output channels, 5x5 kernel\n",
    "        self.pool = nn.MaxPool2d(2) # 2x2 max pooling\n",
    "        self.fc = nn.Linear(320, 10) # Fully connected layer\n",
    "        \n",
    "    #def forward(self, x):\n",
    "    #    x = self.pool(torch.relu(self.conv1(x)))\n",
    "    #    x = self.pool(torch.relu(self.conv2(x)))\n",
    "    #    x = x.view(-1, 320)\n",
    "    #    return self.fc(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ## convolutional layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        ## convolutional layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        ## flattening   \n",
    "        x = x.view(-1, 320)\n",
    "\n",
    "        ## fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367f26a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading the MNIST dataset in Python\n",
    "- We will use the ```torchvision``` package to load the MNIST dataset\n",
    "- The dataset consists of 60,000 training images and 10,000 test images\n",
    "- Each image is a 28x28 pixel grayscale image\n",
    "- The images are labeled with the corresponding digit (0-9)\n",
    "- We will use a batch size of 64 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c104680d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T21:23:18.891832Z",
     "start_time": "2024-11-11T21:23:18.839834Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30f880d756cc3b02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T21:24:32.226613Z",
     "start_time": "2024-11-11T21:24:32.151885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 5')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display a sample image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_dataset[523][0].squeeze(), cmap='gray')\n",
    "plt.title(f'Label: {train_dataset[523][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d71db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instantiate the model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00ac1c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T19:38:57.719265Z",
     "start_time": "2024-11-06T19:38:57.698304Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input shape: (1, 1, 28, 28)\n",
      "Conv2d: (1, 1, 28, 28) -> (1, 10, 24, 24)\n",
      "MaxPool2d: (1, 10, 24, 24) -> (1, 10, 12, 12)\n",
      "Conv2d: (1, 10, 12, 12) -> (1, 20, 8, 8)\n",
      "MaxPool2d: (1, 20, 8, 8) -> (1, 20, 4, 4)\n",
      "Linear: (1, 320) -> (1, 10)\n",
      "Model output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Show per-layer input/output shapes using a dummy MNIST-sized batch\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "print(f\"Model input shape: {tuple(example_input.shape)}\")\n",
    "\n",
    "hooks = []\n",
    "\n",
    "def print_io_shapes(module, inp, out):\n",
    "    inp_shape = tuple(inp[0].shape) if isinstance(inp, (list, tuple)) else tuple(inp.shape)\n",
    "    out_shape = tuple(out.shape)\n",
    "    print(f\"{module.__class__.__name__}: {inp_shape} -> {out_shape}\")\n",
    "\n",
    "for layer in model.children():\n",
    "    hooks.append(layer.register_forward_hook(print_io_shapes))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(example_input)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "print(f\"Model output shape: {tuple(output.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faeb442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b701326",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining a training function\n",
    "- To make the code modular, let's define a function for training the model\n",
    "- The function will take the following arguments:\n",
    "    - The model object\n",
    "    - The training dataloader object\n",
    "    - The optimizer object\n",
    "    - The epoch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60b54cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T19:41:34.967880Z",
     "start_time": "2024-11-06T19:41:34.960759Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) # make a prediction\n",
    "        loss = criterion(output, target) # compute the loss\n",
    "        loss.backward() # compute the gradients\n",
    "        optimizer.step() # update the model parameters\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1333518",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining a test function\n",
    "- Similarily, let's define a function for testing the model\n",
    "- Instead of performing gradient descent and updating the model parameters, this function will be tasked with making predictions on the test set and measuring the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fc8af40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T19:43:05.229140Z",
     "start_time": "2024-11-06T19:43:05.222780Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.2f}%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc726b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training and evaluating the model\n",
    "- In the following code block, we will train the model for 3 epochs\n",
    "- At the end of each epoch, we will evaluate the model on the test set and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a98f6df4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T19:43:58.272753Z",
     "start_time": "2024-11-06T19:43:29.459598Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308833\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.317974\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.352311\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.302690\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.320212\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.148844\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.126616\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.108536\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.126598\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.165842\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9596/10000 (95.96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.072947\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.144156\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.058906\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.047823\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.174106\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.026996\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.087614\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.089711\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.042484\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089455\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9793/10000 (97.93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.012393\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.068480\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.092372\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.051712\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.108154\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.026958\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.065812\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.156789\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.040065\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.121567\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9797/10000 (97.97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 3 epochs\n",
    "for epoch in range(1, 4):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "bmei9400b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
