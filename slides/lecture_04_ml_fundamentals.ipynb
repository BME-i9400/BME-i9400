{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa01178",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Machine Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a59e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning \n",
    "- The most common form of machine learning\n",
    "- The model is trained (i.e., \"supervised\") to provide the correct output for a given input\n",
    "- The training is done using a *labeled* dataset of input-output pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6de25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning (cont'd)\n",
    "- $y = f(x) + e$\n",
    "- $y$: the desired output of the model, also known as the *target* or *label*\n",
    "- $x$: the variables used by the model to make predictions, also known as the *features* or *predictors*\n",
    "- $e$: the error term, also known as the *noise*\n",
    "- $f$: the function whose parameters that we are trying to learn, also known as the *model*\n",
    "    - The noise term $e$ comprises noise in the features, noise in the labels, variables that are not accounted for in the set of features, as well as an overly simplistic function $f$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58583d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forms of supervised learning\n",
    "- **Regression**: the target variable is continuous\n",
    "- Examples of regression problems\n",
    "    - Predicting the price of a house based on its specifications\n",
    "    - Predicting the age of an embryo based on ultrasound image\n",
    "    - Predicting the future value of a stock based on historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab0a1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forms of supervised learning (cont'd)\n",
    "- **Classification**: the target is categorical\n",
    "- Examples of classification problems\n",
    "    - Predicting whether an email is spam or not\n",
    "    - Predicting whether a patient has an arrhythmia based on an ECG\n",
    "    - Transcribing speech to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0807b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elements of a supervised learning problem\n",
    "- **features** $x$: the data that we will use to make predictions\n",
    "- **target** $y$: the variable that we are trying to predict\n",
    "- **model** $f$: the mathematical relationship between features and target (a function whose parameters that we are trying to learn)\n",
    "    - We specify the type of function (e.g. linear, polynomial, Gaussian)\n",
    "- **loss function** $L$: a measure of how good the model's predictions are compared to the targets (the ground-truth) \n",
    "- **optimization method**: a procedure for finding the model that minimizes the loss function (typically some form of gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fa7db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A machine learning dataset\n",
    "- A dataset is a collection of data points\n",
    "- $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(N)}, y^{(N)})\\}$\n",
    "- $N$: the number of data points in the dataset\n",
    "- $x^{(i)}$: the features of the $i$-th data point\n",
    "    - This is generally a column vector of $D$ values \n",
    "    - For example, it may be the vector of pixel intensity values of an ultrasound image\n",
    "- $y^{(i)}$: the target value of the $i$-th data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed678f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training and test datasets\n",
    "- The dataset is typically split into two parts: training and test datasets\n",
    "- Common split ratios are 80/20 and 70/30 (training set is larger than the test set)\n",
    "- **Training dataset**: used to train the model\n",
    "- **Test dataset**: used to evaluate the model's performance on \"unseen\" data (*unseen to the model during training*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336fa87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple case: linear model with a single feature\n",
    "- $\\hat{y} = w_0 + w_1 x_1$\n",
    "- $\\hat{y}$: the model's prediction of the target value $y$\n",
    "- $w_0$: the intercept of the linear model\n",
    "- $w_1$: the slope of the linear model\n",
    "- $x_1$: the feature (just one in this case, so $D=1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6bb9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining feature and intercept term\n",
    "- We can combine the feature $x$ and the intercept $w_0$ into a single *feature vector*\n",
    "- $\\hat{y} = w_0 + w_1 x = w_0 \\cdot 1 + w_1 \\cdot x_1$\n",
    "- $\\hat{y} = w^T x$\n",
    "- $w = [w_0, w_1]$\n",
    "- $x = [1, x_1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb05a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional predictors\n",
    "- If we have more than one feature, we simply add it to the feature vector:\n",
    "- $x = [1, x_1, x_2, \\ldots, x_D]$\n",
    "- In this case, our model will have $D+1$ coefficients:\n",
    "- $w = [w_0, w_1, w_2, \\ldots, w_D]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560ad66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mean Squared Error (MSE) loss function\n",
    "- The loss function measures how well the model's predictions approximate the targets on the training dataset\n",
    "---\n",
    "<center> $L(w) = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2$ </center>\n",
    "\n",
    "---\n",
    "- Note that the loss function is a function of the model's parameters $w$\n",
    "    - Where does $w$ enter into the equation above?\n",
    "- The goal of supervised learning is to find the value of $w$ that minimizes the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449d6f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimizing the loss function\n",
    "- We can use gradient descent to find the optimal values of $w$\n",
    "- The gradient of the loss function with respect to $w$ is: \n",
    "---\n",
    "<center> $\\nabla_w L(w) = \\frac{2}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)}) x^{(i)}$ </center>\n",
    "\n",
    "---\n",
    "- where $\\hat{y}^{(i)} = w^T x^{(i)}$ whose gradient is $\\nabla_w \\hat{y}^{(i)} = x^{(i)}$\n",
    "    - Verify this for yourself (fun!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f5d32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent update rule\n",
    "---\n",
    "<center> $w \\leftarrow w - \\mu \\nabla_w L(w)$ </center>\n",
    "\n",
    "---\n",
    "- $\\mu$: the learning rate, a so-called **hyperparameter** that determines the size of the step that we take in the direction of the negative gradient\n",
    "- The term hyperparameter is used to distinguish $\\mu$ from the parameters of the model $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87a65b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch versus stochastic gradient descent\n",
    "- **Batch gradient descent**: the gradient is computed over the entire training dataset\n",
    "\n",
    "---\n",
    "<center> $w_{\\mathrm{new}} = w_{\\mathrm{old}} - \\mu \\frac{2}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)}) x^{(i)}$ </center>\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "**Stochastic gradient descent**: the gradient is computed over a single data point\n",
    "\n",
    "<center> $w_{\\mathrm{new}} = w_{\\mathrm{old}}  - 2 \\mu (y^{(i)} - \\hat{y}^{(i)}) x^{(i)}$ </center>\n",
    "\n",
    "---\n",
    "- In stochastic gradient descent, we perform $N$ updates of the model for every pass through the data\n",
    "- These passes are called **epochs** in the parlance of modern machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67688571",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "- In the case of a linear model with a MSE loss, we have a closed-form solution\n",
    "- $w = (X^T X)^{-1} X^T y$\n",
    "- $X = [x^{(1)}, x^{(2)}, \\ldots, x^{(N)}]^T$ is the $N$-by-$2$ matrix of features\n",
    "    - The first column of $X$ is a column of ones\n",
    "    - The second column of $X$ is the first feature $x_1$ for each training example\n",
    "    - The third column of $X$ is the second feature $x_2$ for each training example\n",
    "    - The last column of $X$ is the $D$-th feature $x_D$ for each training example\n",
    "- $y = [y^{(1)}, y^{(2)}, \\ldots, y^{(N)}]$ is the $N$-by-$1$ vector of target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf79f53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deriving the closed-form solution\n",
    "- The loss function over the training set can be written as:\n",
    "---\n",
    "<center> $L(w) = \\frac{1}{N} (y - Xw)^T (y - Xw)$ </center>\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "<center> $\\nabla_w L(w) = - \\frac{1}{N} \\left( 2 X^T y - 2 X^T X w \\right)  $ </center>\n",
    "\n",
    "---\n",
    "\n",
    "- Setting the gradient to zero gives:\n",
    "---\n",
    "\n",
    "<center> $X^T X w = X^T y$ </center>\n",
    "<center> $w = (X^T X)^{-1} X^T y$ </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fe191",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moore-Penrose pseudoinverse\n",
    "- The operator $(X^T X)^{-1} X^T$ is known as the **Moore-Penrose pseudoinverse** of $X$\n",
    "- Given a linear system $Ax = b$, the least-squares solution is given by:\n",
    "---\n",
    "<center> $x^{\\ast} = A^+ b$ </center>\n",
    "\n",
    "---\n",
    "- where $A^+ = (A^T A)^{-1} A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca939a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization \n",
    "- **Generalization**: the ability of a model to make accurate predictions on new, unseen data\n",
    "- We use the test set to evaluate a model's ability to generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286abf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting\n",
    "- **Overfitting** occurs when a model is too complex for the problem at hand\n",
    "    - May be thought of as the model learning not just the signal but also the noise\n",
    "- Diagnosed by comparing the model's performance on the training and test sets\n",
    "- If the test set performance is significantly worse than the training set performance, this indicates overfitting\n",
    "- The risk of overfitting is mitigated by training the model with many examples: pairs of $(x, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08bce5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial regression\n",
    "- A linear model can be augmented to include higher-order terms\n",
    "- For example, a quadratic model has the form:\n",
    "---\n",
    "<center> $\\hat{y} = w_0 + w_1 x + w_2 x^2$ </center>\n",
    "\n",
    "---\n",
    "- A cubic model has the form:\n",
    "---\n",
    "<center> $\\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3$ </center>\n",
    "\n",
    "---\n",
    "   - The feature vector is $x = [1, x, x^2, x^3]$\n",
    "   - The model parameters are $w = [w_0, w_1, w_2, w_3]$\n",
    "    - The prediction of the model is $\\hat{y} = w^T x$\n",
    "- Note that, by including higher order terms in the feature vector, the model can now account for non-linear relationships between the features and the target\n",
    "- Importantly, the model that is being learned is still linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b32ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of polynomial regression\n",
    "- We will use a simulated dataset of body temperature fluctuations over 24 hours\n",
    "- The temperature fluctuates due to the circadian rhythm\n",
    "- We will model the data using a polynomial regression model\n",
    "- The goal is to predict the body temperature at any given time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387566a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "97e9ebd7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('body_temperature_data.csv')\n",
    "x = data['time'].to_numpy()\n",
    "y = data['temperature'].to_numpy()\n",
    "\n",
    "x.shape, y.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d9f2194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Displaying the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c1ff545",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='Simulated Body Temperature', marker='o', color='steelblue', linestyle='')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Body Temperature (Celsius)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ff9c637a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the functional form relating time to temperature?\n",
    "- Relatedly, how many degrees should be in our polynomial model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c10fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating our polynomial feature vector"
   ]
  },
  {
   "cell_type": "code",
   "id": "10867dd8",
   "metadata": {},
   "source": [
    "x3 = x ** 3\n",
    "x2 = x ** 2\n",
    "x1 = x ** 1\n",
    "x0 = x ** 0\n",
    "x = np.stack( (x3,x2,x1,x0), axis=1)\n",
    "print(f\"The shape of x is: {x.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "316ffec8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split the data into training and test sets\n",
    "- For simplicity, we will take the first 20 examples as the training set and the remaining 5 examples as the test set"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6e1ca6f",
   "metadata": {},
   "source": [
    "x_train = x[:20,:] # make sure that you understand the syntax!\n",
    "y_train = y[:20]\n",
    "\n",
    "x_test = x[20:,:]\n",
    "y_test = y[20:]\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5cbc2f67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit a polynomial regression model to our training data\n",
    "- We will compute the closed-form solution with the pseudoinverse"
   ]
  },
  {
   "cell_type": "code",
   "id": "43135b85",
   "metadata": {},
   "source": [
    "w = np.linalg.pinv(x_train) @ y_train\n",
    "print(f\"The coefficients of the polynomial model are: {w}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "107a6675",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "id": "1aa77391",
   "metadata": {},
   "source": [
    "yhat_test = x_test @ w"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e7835247",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize the training and test sets\n",
    "- We will plot the training data, the test data, and the model's predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d446d64",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_train[:,2], y_train, label='Training Set', marker='o', color='steelblue', linestyle='')\n",
    "plt.plot(x_test[:,2], y_test, label='Test Set', marker='o', color='darkorange', linestyle='')\n",
    "plt.plot(x_test[:,2], yhat_test, label='Predictions on Test Set', color='darkorange', linestyle='-')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4b19915",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How well did our model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d4d25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data generation below"
   ]
  },
  {
   "cell_type": "code",
   "id": "89759eaa",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Time range over 24 hours (e.g., in hours)\n",
    "time_hours = np.linspace(0, 24, 25)\n",
    "\n",
    "# Amplitude of the body temperature fluctuation (in degrees Celsius)\n",
    "amplitude = 0.5\n",
    "\n",
    "# Baseline body temperature (in degrees Celsius)\n",
    "baseline_temp = 37.0\n",
    "\n",
    "# Period of the circadian rhythm (roughly 24 hours)\n",
    "period = 24\n",
    "\n",
    "# Noise level (standard deviation)\n",
    "noise_level = 0.2\n",
    "\n",
    "# Simulated body temperature as a function of time with sinusoidal circadian rhythm and noise\n",
    "temperature = baseline_temp + amplitude * np.sin(2 * np.pi * time_hours / period) + np.random.normal(0, noise_level, time_hours.shape)\n",
    "\n",
    "# Plot the simulated data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_hours, temperature, label='Simulated Body Temperature', marker='o', color='steelblue', linestyle='')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Body Temperature (Celsius)')\n",
    "plt.title('Simulated Body Temperature Fluctuations Due to Circadian Rhythm')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "df = pd.DataFrame({'time': time_hours, 'temperature': temperature})\n",
    "df.to_csv('body_temperature_data.csv', index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
