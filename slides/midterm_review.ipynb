{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a761497",
   "metadata": {},
   "source": [
    "# BME I9400 Machine Learning Midterm Review Notebook\n",
    "\n",
    "---\n",
    "## 1. Bayes' Rule and Conditional Probability\n",
    "\n",
    "### Concept Recap\n",
    "Bayes' theorem connects prior and conditional probabilities:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "It allows us to update our beliefs about event A given evidence B.\n",
    "\n",
    "A = disease (1) or no disease (0)\n",
    "B = test positive (1) or negative (0)\n",
    "\n",
    "### Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "Suppose 1% of people have a disease, and a test is 99% sensitive and 95% specific.\n",
    "P_disease = 0.01\n",
    "P_pos_given_disease = 0.99\n",
    "P_pos_given_no_disease = 0.05\n",
    "\n",
    "P_positive = P_pos_given_disease * P_disease + P_pos_given_no_disease * (1 - P_disease)\n",
    "P_disease_given_positive = (P_pos_given_disease * P_disease) / P_positive\n",
    "\n",
    "print(f\"P(Disease | Positive Test) = {P_disease_given_positive:.3f}\")\n",
    "```\n",
    "\n",
    "### Questions\n",
    "1. What is the probability of having the disease if you test positive?\n",
    "2. How would you compute $P(B|A)$ if you only have sample data?\n",
    "\n",
    "---\n",
    "## 2. Feature Matrices, Vectors, and Targets\n",
    "\n",
    "### Concept Recap\n",
    "- **Feature matrix** \\( X \\in \\mathbb{R}^{n \\times d} \\): n examples, d features\n",
    "- **Target vector** \\( y \\in \\mathbb{R}^n \\)\n",
    "\n",
    "### Example\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 3 examples, 2 features each\n",
    "X = np.array([[1.0, 2.0],\n",
    "              [2.0, 0.5],\n",
    "              [3.0, 4.0]])\n",
    "y = np.array([1, 0, 1])\n",
    "\n",
    "print(\"Feature matrix X:\\n\", X)\n",
    "print(\"Target vector y:\\n\", y)\n",
    "```\n",
    "\n",
    "### Practice Questions\n",
    "1. How would you represent a bias term in this matrix form?\n",
    "2. What happens to the shape of X if you standardize each feature?\n",
    "\n",
    "---\n",
    "## 3. Loss Functions (MSE, BCE)\n",
    "\n",
    "### Concept Recap\n",
    "- **Mean Squared Error (MSE):**\n",
    "  $$ L_{MSE} = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "- **Binary Cross Entropy (BCE):**\n",
    "  $$ L_{BCE} = -\\frac{1}{n}\\sum_i [y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)] $$\n",
    "\n",
    "### Example\n",
    "```python\n",
    "y_true = np.array([0, 1, 1, 0])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.3])\n",
    "\n",
    "```\n",
    "\n",
    "### Practice Questions\n",
    "1. What is the MSE on this sample dataset?\n",
    "2. What is the BCE on this sample dataset?\n",
    "\n",
    "---\n",
    "## 4. Linear Regression\n",
    "\n",
    "### Concept Recap\n",
    "$$ \\hat{y} = Xw + b $$\n",
    "The parameters \\( w, b \\) minimize MSE.\n",
    "\n",
    "### Example\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2.2, 2.8, 4.5, 4.2])\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"Coefficient:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Predictions:\", model.predict(X))\n",
    "```\n",
    "\n",
    "### Practice Question\n",
    "- Assuming no intercept term, and knowing that the least squares solution is $w^{\\ast} (X^{T} X)^{-1} X^T * y$, what is the optimal regression coefficient $w$ for this toy dataset>\n",
    "\n",
    "---\n",
    "## 5. Logistic Regression\n",
    "\n",
    "### Concept Recap\n",
    "$$ \\hat{y} = \\sigma(Xw + b) = \\frac{1}{1 + e^{-(Xw+b)}} $$\n",
    "\n",
    "### Example\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = np.array([[0], [1], [2], [3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "log_model = LogisticRegression().fit(X, y)\n",
    "print(\"Weights:\", log_model.coef_)\n",
    "print(\"Intercept:\", log_model.intercept_)\n",
    "print(\"Predictions:\", log_model.predict_proba(X))\n",
    "```\n",
    "\n",
    "### Practice Question\n",
    "1. Whatâ€™s the geometric interpretation of the decision boundary? (It helps to visualize the dataset)\n",
    "\n",
    "---\n",
    "## 6. Gradient Descent and Optimization\n",
    "\n",
    "### Concept Recap\n",
    "Gradient descent iteratively updates parameters:\n",
    "\n",
    "$$ w \\leftarrow w - \\eta \\nabla_w L(w) $$\n",
    "\n",
    "### Example: Simple quadratic loss\n",
    "```python\n",
    "w = 5.0  # initial guess\n",
    "eta = 0.1\n",
    "for i in range(10):\n",
    "    grad = 2 * (w - 3)\n",
    "    w -= eta * grad\n",
    "    print(f\"Iter {i}: w = {w:.3f}\")\n",
    "```\n",
    "\n",
    "### Discussion\n",
    "1. Why is the update rule $$ w \\leftarrow w - \\eta \\nabla_w L(w) $$ and not $$ w \\leftarrow w + \\eta \\nabla_w L(w) $$ ?\n",
    "2. What issues arise if the learning rate is too large or too small?\n",
    "\n",
    "---\n",
    "## 7. Overfitting, Regularization (L1/L2)\n",
    "\n",
    "### Concept Recap\n",
    "- **Overfitting:** Low training error, high test error.\n",
    "- **Regularization:** Adds penalty to weights.\n",
    "\n",
    "$$ L_{\\mathrm{L2}} = MSE + \\lambda ||w||_2^2 $$  (L2)\n",
    "$$ L_{\\mathrm{L1}} = MSE + \\lambda ||w||_1 $$  (L1)\n",
    "\n",
    "### Example\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "X = np.random.randn(20, 3)\n",
    "y = X @ np.array([1.5, -2.0, 0.5]) + np.random.randn(20)*0.3\n",
    "\n",
    "ridge = Ridge(alpha=1.0).fit(X, y)\n",
    "lasso = Lasso(alpha=0.1).fit(X, y)\n",
    "\n",
    "print(\"Ridge coefficients:\", ridge.coef_)\n",
    "print(\"Lasso coefficients:\", lasso.coef_)\n",
    "```\n",
    "\n",
    "### Stretch Questions\n",
    "1. Why does L1 encourage sparsity in coefficients?\n",
    "2. Why is L2 regularization sometimes called ``shrinkage''?\n",
    "\n",
    "---\n",
    "## 8. Cross Validation\n",
    "\n",
    "### Concept Recap\n",
    "- **Cross-validation** splits data into training and validation sets to estimate generalization.\n",
    "\n",
    "### Example\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.random.randn(50, 2)\n",
    "y = X @ np.array([1.0, 2.0]) + np.random.randn(50)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(LinearRegression(), X, y, cv=cv, scoring='r2')\n",
    "print(\"Cross-validated R^2 scores:\", scores)\n",
    "print(\"Mean R^2:\", np.mean(scores))\n",
    "```\n",
    "\n",
    "### Discussion\n",
    "1. Why is k-fold CV preferred over a single train/test split?\n",
    "2. How might you use CV to select hyperparameters such as \\( \\lambda \\) in regularization?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmei9400b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
