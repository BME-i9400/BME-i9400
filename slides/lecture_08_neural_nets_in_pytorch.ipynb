{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BME i9400\n",
    "## Fall 2024\n",
    "### Neural Networks in PyTorch\n"
   ],
   "id": "fd38c16f6e760cf6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classes and objects in Python\n",
    "- An ```object``` is a structure that contains both data (variables) and functions (methods)\n",
    "- A ```class``` is a blueprint for creating objects\n",
    "- Classes are used to organize programs and make them more modular"
   ],
   "id": "dc6c8b521801667a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example: Medical record class\n",
    "- We will create a class to represent a patient's medical record\n",
    "- The class will have the following attributes:\n",
    "    - Name\n",
    "    - Age\n",
    "    - Height\n",
    "    - Weight\n",
    "    - BMI\n",
    "- The class will have the following methods:\n",
    "    - ```calculate_bmi```: calculates the BMI of the patient\n",
    "    - ```print_record```: prints the patient's record\n",
    "    - ```update_weight```: updates the patient's weight\n",
    "    - ```update_height```: updates the patient's height\n",
    "    - ```update_age```: updates the patient's age\n",
    "    - ```update_name```: updates the patient's name\n",
    "    - ```update_record```: updates the patient's record"
   ],
   "id": "b4ede7219a5ca06d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T16:52:14.379998Z",
     "start_time": "2024-11-04T16:52:14.374230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MedicalRecord:\n",
    "    def __init__(self, name, age, height, weight):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.height = height\n",
    "        self.weight = weight\n",
    "        self.bmi = self.calculate_bmi()\n",
    "\n",
    "    def calculate_bmi(self):\n",
    "        return self.weight / (self.height**2)\n",
    "\n",
    "    def print_record(self):\n",
    "        print(f'Name: {self.name}')\n",
    "        print(f'Age: {self.age}')\n",
    "        print(f'Height: {self.height}')\n",
    "        print(f'Weight: {self.weight}')\n",
    "        print(f'BMI: {self.bmi}')\n",
    "\n",
    "    def update_weight(self, weight):\n",
    "        self.weight = weight\n",
    "        self.bmi = self.calculate_bmi()\n",
    "\n",
    "    def update_height(self, height):\n",
    "        self.height = height\n",
    "        self.bmi = self.calculate_bmi()\n",
    "\n",
    "    def update_age(self, age):\n",
    "        self.age = age\n",
    "\n",
    "    def update_name(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def update_record(self, name, age, height, weight):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.height = height\n",
    "        self.weight = weight\n",
    "        self.bmi = self.calculate_bmi()"
   ],
   "id": "ab175e0ce4811633",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating an object of the MedicalRecord class",
   "id": "2563bda05ecec1d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T16:57:24.677083Z",
     "start_time": "2024-11-04T16:57:24.674782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "patient = MedicalRecord('Jacek', 29, 1.77, 73)\n",
    "# patient is an instance of the MedicalRecord class (an object)"
   ],
   "id": "6deab8f87d8c64c5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Obtaining *attributes* of a class",
   "id": "51091badb893646b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T16:55:26.383692Z",
     "start_time": "2024-11-04T16:55:26.381199Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacek\n",
      "29\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "print(patient.name)\n",
    "print(patient.age)"
   ],
   "id": "993e48d7ac328a3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calling *methods* of a class",
   "id": "fc820f0bbe0edde8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T16:56:01.521241Z",
     "start_time": "2024-11-04T16:56:01.517416Z"
    }
   },
   "cell_type": "code",
   "source": "patient.print_record()",
   "id": "aa94e5dd78d22589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Jacek\n",
      "Age: 29\n",
      "Height: 1.77\n",
      "Weight: 73\n",
      "BMI: 23.301094832264035\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## From theory to practice\n",
    "- We have seen the theory of multilayer perceptrons (MLPs) in the previous lecture\n",
    "- We now turn to the practical aspects of implementing MLPs in PyTorch\n",
    "- We will begin by implementing logistic regression in PyTorch"
   ],
   "id": "5c294f55c365e881"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Library imports",
   "id": "e06693d253744048"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "a25bb4183db340b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generating a synthetic dataset\n",
    "- We will make use of scikit-learn's ```make_classification``` function to generate a simple binary classification dataset\n",
    "- We will split the dataset into training and validation sets\n",
    "- We will then convert the NumPy arrays to PyTorch tensors"
   ],
   "id": "3f971fb6fdc5ee2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate features and labels\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert features and labels into PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)"
   ],
   "id": "c41a84519ff00e31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Logistic regression in PyTorch\n",
    "- A neural network in PyTorch is an instance of the ```nn.Module``` class\n",
    "- Each neural network consists of a series of *layers*\n",
    "- In our simple case, we will use a single layer for logistic regression\n",
    "- Each nn.Module subclass must implement a ```forward``` method that defines the forward pass of the network\n",
    "- Remember that ``forward pass'' means computing the output of the network given the input"
   ],
   "id": "9e0b7ee6ffe1193e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the logistic regression model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size): # input_size: number of features, output_size: number of classes\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # Single layer for logistic regression\n",
    "        self.linear = nn.Linear(input_size, output_size) # \"linear\" is the *name* of our layer\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax activation function to convert outputs to probabilities\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ],
   "id": "7b77720d49b2a555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating an instance of the model\n",
    "- So far we have defined a class, which is a blueprint for the model\n",
    "- To actually create a model that we can work with, we need to *instantiate* the class\n",
    "- Instantiating a class means creating an *object* of that class"
   ],
   "id": "b26d2cbc993494b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_size = X_train.shape[1]  # Number of features\n",
    "output_size = 2  # Binary classification\n",
    "model = LogisticRegressionModel(input_size, output_size)"
   ],
   "id": "20ec570bcfea3422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inspecting the model\n",
    "- The model object provides a convenient way to access the layers and parameters of the model\n",
    "- Let's take a look at the weights and biases of the model"
   ],
   "id": "977c351acaba7fa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.linear.weight",
   "id": "d12a297ca2a63f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.linear.bias",
   "id": "d45feb1037351aec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Q1: Why are there two rows in the weight matrix?\n",
    "### Q2: Why are there two columns in the weight matrix?\n",
    "### Q3: Where did the weights and biases come from?"
   ],
   "id": "a28d7839db80e700"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluating the (untrained) model on random inputs\n",
    "- Before training the model, let's see what the model predicts on random inputs\n",
    "- This will help us to understand how the forward pass works in PyTorch"
   ],
   "id": "ef3beefa88ac351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random input\n",
    "x = torch.randn(1, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "output"
   ],
   "id": "eb1aa6dae9a57c0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluating model on a batch of inputs\n",
    "- A *batch* refers to a set of examples that is used to compute the gradient of the loss function during training\n",
    "- A typical batch size is 32, 64, or 128 examples\n",
    "- An *epoch* refers to a complete pass through the dataset, such that each examples has appeared in a batch once"
   ],
   "id": "fce00d70720f7b12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random batch of inputs\n",
    "X_batch = torch.randn(5, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(X_batch)\n",
    "output"
   ],
   "id": "f853b2e9463214b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining the loss function and optimizer\n",
    "- PyTorch provides a wide range of loss functions and optimizers\n",
    "- For logistic regression, we will use the cross-entropy loss and stochastic gradient descent (SGD) optimizer\n",
    "- An important parameter for the optimizer is the *learning rate*, which controls the size of the updates to the weights"
   ],
   "id": "9556bd01e3509d65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # learning rate = 0.01"
   ],
   "id": "66deaba6ceaeebb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the model\n",
    "- We are now ready to run our stochastic gradient descent (SGD) algorithm to train the model\n",
    "- For simplicity, we will use all of the data in a single batch (batch size = 100 = number of training examples)\n",
    "- We will train over 100 epochs\n",
    "- At each epoch, we will compute the loss, compute the gradients, and update the weights"
   ],
   "id": "f8af062ef07236b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the logistic regression model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ## Forward pass\n",
    "    \n",
    "    # Compute the predicted outputs\n",
    "    outputs = model(X_train)\n",
    "    \n",
    "    # Compute the loss function using the current predictions\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    ## Backward pass\n",
    "\n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad() # zero them from the previous iteration\n",
    "    loss.backward() # compute the gradients!\n",
    "    optimizer.step() # update the weights!\n",
    "\n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ],
   "id": "af2ed26b2ac317bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluating the trained model\n",
    "- We can now evaluate the trained model on the validation set"
   ],
   "id": "752dc8f23d5dd6d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the predicted outputs\n",
    "outputs = model(X_val)\n",
    "\n",
    "# compute the estimated probabilities of the positive class\n",
    "py_hat_val = outputs[:,1]\n",
    "\n",
    "# convert the probabilities to numpy format\n",
    "py_hat_val = py_hat_val.detach().numpy()\n",
    "\n",
    "# measure the area under the ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_val, py_hat_val)\n"
   ],
   "id": "4a87a1a2ab00fbbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How did we do relative to random guessing?",
   "id": "d97c1ad4bce1df9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing an MLP with a single hidden layer\n",
    "- We will now implement a multilayer perceptron (MLP) with a single hidden layer\n",
    "- For this, we will need to define a new class that inherits from ```nn.Module```"
   ],
   "id": "d42521684cee8496"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SingleLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SingleLayerMLP, self).__init__()\n",
    "        # First layer (input to hidden)\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        # Activation function (ReLU introduces non-linearity)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Output layer (hidden to output)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        # softmax activation for output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the first layer, apply ReLU, then pass to output layer and apply softmax\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "id": "750d49766bb3d2b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating an instance of the single-layer MLP model\n",
    "- We need to select the number of units in our hidden layer\n",
    "- Remember that this is called a *hyperparameter* because it is not learned from the data\n",
    "- We will begin with an arbitrary choice of 5 hidden units"
   ],
   "id": "9b23a3d4bc816878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden_size = 5  # Choose an arbitrary number of neurons for the hidden layer\n",
    "model = SingleLayerMLP(input_size, hidden_size, output_size)"
   ],
   "id": "85c56744e22fa0c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inspecting the model architecture is good practice",
   "id": "d15fd4bb4c36acea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(model)",
   "id": "21623d4d342b6c0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define the optimizer and loss function (same as before)\n",
    "- Now we will train our single-layer MLP model using the same cross-entropy loss and stochastic gradient descent optimizer"
   ],
   "id": "ba81f5add8d5e819"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "id": "7899d49b0c398bef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the single-layer MLP model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ],
   "id": "23be144e66d07721",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating the trained model",
   "id": "f2b4d007caf1d7b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the predicted outputs\n",
    "outputs = model(X_val)\n",
    "\n",
    "# compute the estimated probabilities of the positive class\n",
    "py_hat_val = outputs[:,1]\n",
    "\n",
    "# convert the probabilities to numpy format\n",
    "py_hat_val = py_hat_val.detach().numpy()\n",
    "\n",
    "roc_auc_score(y_val, py_hat_val)"
   ],
   "id": "dbb835ea10a7583",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How did we do relative to logistic regression?",
   "id": "3f414b270d1e4206"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating an MLP with an arbitrary number of hidden layers\n",
    "- PyTorch makes it easy to create MLPs with an arbitrary number of hidden layers\n",
    "- We will create another class inheriting from ```nn.Module``` that allows us to specify the number of hidden layers\n",
    "- In the code below, the list ```hidden_sizes``` contains the number of neurons in each hidden layer\n",
    "- The number of elements in this list defines the number of hidden layers"
   ],
   "id": "73cdd467cb45130e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MultiLayerMLP, self).__init__()\n",
    "        # Define the layers sequentially\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h in hidden_sizes: # hidden_sizes \n",
    "            # Add a hidden layer with ReLU activation\n",
    "            layers.append(nn.Linear(in_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = h\n",
    "        # Add the final output layer\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        # Combine layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.model(x))"
   ],
   "id": "86dd47adb3c3edb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating an instance of the multi-layer MLP model",
   "id": "8c1bcd2b1a3e52f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden_sizes = [10, 5]  # Two hidden layers with 10 and 5 neurons, respectively\n",
    "model = MultiLayerMLP(input_size, hidden_sizes, output_size)\n",
    "print(model)"
   ],
   "id": "baa73affc747b41a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model (same as before)",
   "id": "bc1455f1bcbdc5a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "id": "9623ee210e079b20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ],
   "id": "6ecbeaceb65f015e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating the trained model",
   "id": "5ab0c7309190c594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the predicted outputs\n",
    "outputs = model(X_val)\n",
    "\n",
    "# compute the estimated probabilities of the positive class\n",
    "py_hat_val = outputs[:,1]\n",
    "\n",
    "# convert the probabilities to numpy format\n",
    "py_hat_val = py_hat_val.detach().numpy()\n",
    "\n",
    "roc_auc_score(y_val, py_hat_val)"
   ],
   "id": "49f94b33a18f72c2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
